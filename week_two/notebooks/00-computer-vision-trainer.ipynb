{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision Base Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic steps we'll take are:\n",
    "\n",
    "1. Importing our collected, organized, and cleaned images\n",
    "1. Fine-tune a pretrained neural network to recognise these two groups\n",
    "1. Try running this model on a picture from our test dataset and see if it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: Ensure you are running this notebook within a Docker Container\n",
    "\n",
    "View the README.md file for instructions on building and running a container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Import the image data to the workspace so we can use them to train a model\n",
    "\n",
    "The Docker container will create a nice `workspace` for us that we can recreate if we ever need to start over from scratch. This can allow us to try many different things from the same starting point if we want.\n",
    "\n",
    "The folder structure for this workspace is\n",
    " - data (our collected data will be placed here)\n",
    " - images (other images we may need that is not the data)\n",
    " - models (models we choose to save will be stored here)\n",
    " - notebooks (the notebooks we run can be found in this directory)\n",
    "\n",
    "Within our code, we can use both **absolute** and **relative** paths. This notebook is inside of the notebooks folder, and to reference a file in the data folder we can use the following options below:\n",
    " - absolute path: `/workspace/data/filename.zip`\n",
    " - relative path: `../data/filename.zip` (the `..` in this case means from where this file is, go up one folder)\n",
    "\n",
    "The code below imports `zipfile` and `os` so that we can work with the filesystem. Take a close look at the comments for each line as explanations for what will happen when we run the code in that cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the path to the zip file and the extraction directory\n",
    "zip_file_name = 'TRAINING_DATA' # You will want to change this filename to the filename of your data\n",
    "\n",
    "zip_file_path = f\"/workspace/data/{zip_file_name}.zip\" # You should not need to edit anything here because the zip_file_path references the variable containing the filename above, this code assumes it will be a zip file\n",
    "extract_to_dir = '/workspace/data/' # This is where the extracted zip file will go, turning that one zip file into the directories and images files that we will need to work with for training\n",
    "\n",
    "# Check if the TRAINING_DATA directory already exists\n",
    "if not os.path.exists(extract_to_dir+zip_file_name): # If the directory with the real data doesn't exist, run the code inside this if statement. \n",
    "    # Create the extraction directory if it doesn't exist\n",
    "    os.makedirs(extract_to_dir, exist_ok=True) # Since the directory does not exist, it needs to be created\n",
    "\n",
    "    # Open the zip file and extract its contents\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref: # These two lines extract the data from the zip file, and place them in the defined path for the extract_to_dir variable\n",
    "        zip_ref.extractall(extract_to_dir)\n",
    "\n",
    "    print(f\"Extracted all files to {extract_to_dir}\") # Recall that print statments can be very useful, this print statement let's you know that the extraction process has completed\n",
    "else: # This else block will only run if the directory with the real data already exists. If you need to unzip the data again, you will have to remove this directory inside the /workspace/data/ directory\n",
    "    print(f\"Directory {extract_to_dir} already exists. Skipping extraction.\") # Since this block only runs if the directory already exists, it's helpful to get a message that it's already there\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the size of your zip file, and the amount of data you are importing, this may take a while. But once you have extracted the data, the Docker container is setup so that it will persist (stay around) when you come back to this notebook again.\n",
    "\n",
    "*NOTE: You do not want to run this cell until the zip file has been fully uploaded to the `/workspace/data/` directory. If you see an error that the zip file is not a zip file, but you know that it is, it has not finished uploading yet.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train an initial model so that we clean our data\n",
    "\n",
    "The goal of step 2 is to do a light run of the training in order to clean up our data. While it may seem to make the most sense to clean the data by looking at every picture before we even attempt to train (and you should definitely give your data a look through), cleaning after running a few epochs of fine tuning can be very helpful.\n",
    "\n",
    "We will be using the FastAPI library from Fast.AI to train our models. Documentation for this library can be found at [https://docs.fast.ai/](https://docs.fast.ai/).\n",
    "\n",
    "For our purposes, we will be working within the subset of the API for **Computer Vision**.\n",
    " - This means we do not need to import all of fast ai (`import fastai`).\n",
    " - Instead, we will just need to import the subset of the api for `vision` (`from fastai.vision.all import *`)\n",
    "   - note that the `*` here means everything within `fastai.vision.all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to the tools we will need for training, it can be helpful to set a variable for the `path` to where the data is stored. I often put mine in a directory called *TRAINING_DATA*, but yours should be the same as the name extracted from the zip file you unpacked above\n",
    "\n",
    "For example, if my zipfile was named `IAN_DATA.zip` and created a folder called `IAN_DATA` I would modify the assignment of `path` to be `path = /workspace/data/IAN_DATA`. The `.zip` is dropped here because we are looking at the folder that was created through the unzipping process, and not the compressed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/workspace/data/TRAINING_DATA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the location of the data is set in a variable, we *might* want to verify that all of our data is image data. If you have collected and organized all of the data, and are confident you don't have any random files in the directories, you may not need this. But, it can be helpful to run it once to ensure we won't have filetype issues going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this test if you've added new data to the training data\n",
    "# it does NOT need to be run if you're confident that the image files are valid \n",
    "failed = verify_images(get_image_files(path)) # this looks at all of the files in our defined path and verifies that they are images, and failures will be placed in the failed variable\n",
    "failed.map(Path.unlink) # if there are any failes that were not images, remove them from the data\n",
    "len(failed) # show the number of failed files, ideally this will be 0, but even if it's more than 0 know that those files are now gone and we don't need to worry about them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, we'll need a `DataLoader`, which is an object that contains a *training set* (the images used to create a model) and a *validation set* (the images used to check the accuracy of a model -- not used during training). In `fastai` we can create that easily using a `DataBlock`, and view sample images from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock), \n",
    "    get_items=get_image_files, \n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    get_y=parent_label,\n",
    "    item_tfms=[Resize(192, method='squish')],\n",
    "    batch_tfms=[\n",
    "        *aug_transforms(size=192, min_scale=0.75, p_lighting=0.8, do_flip=true),  # Adjust the p_lighting parameter\n",
    "        Normalize.from_stats(*imagenet_stats)\n",
    "    ]\n",
    ").dataloaders(path, bs=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here what each of the `DataBlock` parameters means:\n",
    "\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "\n",
    "The inputs to our model are images, and the outputs are categories (in this case, \"bird\" or \"forest\"). Meaning we can send it a picture and it will give us some text.\n",
    "\n",
    "    get_items=get_image_files, \n",
    "\n",
    "To find all the inputs to our model, run the `get_image_files` function (which returns a list of all image files in a path).\n",
    "\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "\n",
    "Split the data into training and validation sets randomly, using 20% of the data for the validation set.\n",
    "\n",
    "    get_y=parent_label,\n",
    "\n",
    "The labels (`y` values) is the name of the `parent` of each file (i.e. the name of the folder they're in, which will be the names of each plant).\n",
    "\n",
    "    item_tfms=[Resize(192, method='squish')]\n",
    "\n",
    "Before training, resize each image to 192x192 pixels by \"squishing\" it (as opposed to cropping it).\n",
    "\n",
    "    batch_tfms=[\n",
    "        *aug_transforms(size=192, min_scale=0.75, p_lighting=0.8, do_flip=true),\n",
    "        Normalize.from_stats(*imagenet_stats)\n",
    "    ]\n",
    "\n",
    "For each batch, which is 256 at a time for this code, set the probability of changing the lighting (p_lighting) of the image to 75% and use random flipping (do_flip=true) for each set of data\n",
    "\n",
    "Read more about data augmentations, whether at the item level, or batch level, at [https://docs.fast.ai/vision.augment.html](https://docs.fast.ai/vision.augment.html) (you will want to scroll to the bottom to see a list of these parameters)\n",
    "\n",
    "The code above let's us take a look at how a batch (256 in the code above, `bs=256`) of the data is augmented based on the parameters of the dataloader. \n",
    "You should see images that have been:\n",
    " - squished into 192 x 192 pixel sized squares (`size=192`)\n",
    " - some cropping (`min-scale=0.75`)\n",
    " - a change in the brightness or contrast (`p_lighting=0.8`)\n",
    " - some random flipping (`do_flip=true`)\n",
    "\n",
    "These augmentations allow our data to become more than just what the image originally was. Our model doesn't need to require that the original image was taken upside-down sometimes, but can account for that possibility through the data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(max_n=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train our model. A fast, and widely used computer vision model is `resnet18`, which is a pre-trained model that already works for other images. The `resnet18` model was trained on the ImageNet dataset, which is comprised of significantly more images and categories than we are likely to use in this trainer notebook. It is a *pre-trained* model that we can build upon, instead of needing to develop a model from scratch. The `18` referes to the number of layers in the model. Other options are `resnet34`, `resnet50`, `resnet101`, and `resnet152`. To use any of these other pre-trained models, you simply replace `resnet18` with the one you prefer below in the `vision_learner()` parameters. \n",
    "Example: `learn = vision_learner(dls, resnet101, metrics=error_rate)`\n",
    "\n",
    "More information about pretrained models, and others that you may want to use instead of `resnet18` can be found at [https://www.kaggle.com/code/jhoward/which-image-models-are-best](https://www.kaggle.com/code/jhoward/which-image-models-are-best).\n",
    "\n",
    "**The code below creates a learner, and in this case a vision_learner since we are working within *computer vision.***\n",
    "\n",
    "More information about learners in can be found at [https://docs.fast.ai/learner.html](https://docs.fast.ai/learner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=error_rate) # create the learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a learner, we will want to find an optimal learning rate, which can be thought of as the step size to take during the learning (fine tuning). Too small of a learning rate, and it might take a long time to find the best model for our data. Too large of a learning rate and we might jump right over the best model for our data.\n",
    "\n",
    "We will use the value that the code below returns when we fine tune the model for our dataset.\n",
    "\n",
    "More information about the learning rate can be found at [https://medium.com/@bijil.subhash/transfer-learning-how-to-pick-the-optimal-learning-rate-c8621b89c036](https://medium.com/@bijil.subhash/transfer-learning-how-to-pick-the-optimal-learning-rate-c8621b89c036)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find() # find the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fastai` comes with a helpful `fine_tune()` method which automatically uses best practices for fine tuning a pre-trained model, so we'll use that.\n",
    "\n",
    "\"Fine-tuning\" a model means that we're starting with a model someone else has trained using some other dataset (called the *pretrained model*), and adjusting the weights a little bit so that the model learns to recognise your particular dataset. In this case, the pretrained model was trained to recognise photos in *imagenet*, and widely-used computer vision dataset with images covering 1000 categories) For details on fine-tuning and why it's important, check out the [free fast.ai course](https://course.fast.ai/).\n",
    "\n",
    "**Recall that in this *step* we are training in order to clean the data. 3 epochs should be adequate for this, and it is suggested you don't try to train here as if this is the final step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0014454397605732083 # use the value from the lr_find() method above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_training_epochs = 3\n",
    "learn.fine_tune(initial_training_epochs, base_lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our initial pass has completed, let's look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Let's see how our model did and what it had the most difficulty with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the predictions for our model we will use an interpretation function. Since we are specifically interested in classification for this model (giving it a piece of data and receiving a category label), we will use the `ClassificationInterpretation` class and tell it to interpret from the learner we've been using so far.\n",
    "\n",
    "Documentation can be found at [https://docs.fast.ai/interpret.html](https://docs.fast.ai/interpret.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our interpretation, let's examine a confusion matrix for the model. Running the cell below will show the percentage of that were either correctly, or incorrectly, identified as a particular category. If you would prefer to see the raw numbers of elements, modify the `normalize` parameter to `False`.\n",
    "\n",
    "You can read more about the what a confusion matrix shows at [https://www.geeksforgeeks.org/confusion-matrix-machine-learning/](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(normalize=True, title='Confusion Matrix', figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use our interpretation to show items in our dataset that the model had the most trouble with. The `plot_top_losses` method below may show high confidence (probability) that an image is one category (prediction), but it was actually (actual) organized in the data as another. We will be able to clean up any mislabeled data in Step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, if you want to test out individual files you can use the code below. I suggest creating a new folder called `TEST_IMAGES` in `/workspace/images/` and uploading some files to test there. This should result in an image with the filename `yarrow.jpg` having a full path of `/workspace/images/TEST_IMAGES/yarrow.jpg`.\n",
    "\n",
    "The code below will use that path, show you a thumbnail of the image, and then use `learn.predict()` to return an identified category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = 'yarrow.jpg'\n",
    "img_path = f\"/workspace/images/TEST_IMAGES/{img_file}\"\n",
    "Image.open(img_path).to_thumb(256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category,_,probs = learn.predict(PILImage.create(img_path))\n",
    "print(f\"This is a: {category}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Let's clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we may have identified some bad or mislabeled data. We will use the `ImageClassifierCleaner` function on our learner to let us modify any labels, or potentially remove data, directly from this notebook.\n",
    "\n",
    "You may find that large collections of data may have been mislabeled, which may result in actually modifying the data found in `/workspace/data/`. You should feel free to do this, but you will likely want to run steps 2-3 again on this modified data. Situations like this can occur if multiple sets of data are being put into one large collection of data and end up in the wrong folder.\n",
    "\n",
    "When using the cleaner below, you should know that what you see is organized in order of lowest probability to highest. So the first images are the most likely to be challenging for our model. Using the drop-down tool under each image, you can choose to **keep** that image with that category, **delete** it, or **move it to a new category**. You will want to examine every category, for both the training, and the validation, sets of data. \n",
    "\n",
    "A quick reminder that these were created using the `splitter` parameter in the `DataBlock` earlier. If you have left that code as is, this resulted in 20% of the data being used for the validation set, and 80% being used for training (`splitter=RandomSplitter(valid_pct=0.2, seed=42)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.widgets import ImageClassifierCleaner\n",
    "cleaner = ImageClassifierCleaner(learn)\n",
    "cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have cleaned the data, the code below will use a `for` loop to enact any of the modifications you made on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in cleaner.delete(): cleaner.fns[idx].unlink()\n",
    "for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you are satisfied with your trained `Learner`, then you can export the file that contains all the information needed to use the model elsewhere, whether that be in an application or for futue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('../models/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Now We Train A Model Using What We've Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we have at this point:\n",
    " - A data learner that can run effectively using our chosen parameters\n",
    " - The batch size that let's us optimally utilize the resources on our computer\n",
    " - The learning rate that is effective for the data_learner and data\n",
    " - A cleaned up dataset to work with for training\n",
    "\n",
    "We now know a lot about or data and model, likely enough to run a train for many epochs. Use the variables below to setup your script to:\n",
    " 1. set how many epochs should be run in between saves\n",
    " 1. provide the path to a previously saved model if you want to start from there\n",
    "\n",
    "*Note 1: load_learner requires all your custom code be in the exact same place as when exporting your Learner (the main script, or the module you imported it from). So don't modify this script after you've exported the model.*\n",
    "\n",
    "*Note 2: models may not be small, you will want to account for the amount of space you have on your computer and how many saves you may trigger while training. If your model is 50MB, and you will save 10 times, that turns into 500MB of models stored on your computer.*\n",
    "\n",
    "*Note 3: Notice that the learning rate is not defined here. It has already been set earlier, and we do not need to recreate it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an explanation about what is happening in the code below.\n",
    "\n",
    "Custom Callback:\n",
    " - `SaveModelCallback` is a custom callback that saves the model every `self.every` epochs, appending the epoch number to the file name.\n",
    "\n",
    "Jupyter Notebook:\n",
    " - Load the model and data loaders as before.\n",
    " - Instantiate the `SaveModelCallback` and pass it to `learn.fine_tune` using the `cbs` parameter.\n",
    " - Save the final model at the end.\n",
    "\n",
    "**Good luck with your model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first set how often the model should save\n",
    "save_after_this_many_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.core import Callback\n",
    "\n",
    "class SaveModelCallback(Callback):\n",
    "    def __init__(self, every=10, path='model'):\n",
    "        self.every = every\n",
    "        self.path = path\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Save the model every `self.every` epochs\"\n",
    "        last_epoch_save = 3 # this will help to keep the file labeling accurate, if the model was last run with 3 epochs, add it to this run\n",
    "        if (self.epoch + 1) % self.every == 0:\n",
    "            self.learn.save(f'{self.path}_epoch{self.epoch + (1+last_epoch_save)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import load_learner\n",
    "from fastai.data.core import DataLoaders\n",
    "\n",
    "# Define your data loading function\n",
    "def get_dls(path_to_data):\n",
    "    # Your data loading logic here, it needs to match what you used for the model\n",
    "    # For example, this could be something like:\n",
    "    # dls = ImageDataLoaders.from_folder(path, train='train', valid='valid', ...)\n",
    "    return DataBlock(\n",
    "        blocks=(ImageBlock, CategoryBlock), \n",
    "        get_items=get_image_files, \n",
    "        splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "        get_y=parent_label,\n",
    "        item_tfms=[Resize(192, method='squish')],\n",
    "        batch_tfms=[\n",
    "            *aug_transforms(size=192, min_scale=0.75, p_lighting=0.8, do_flip=true),  # Adjust the p_lighting parameter\n",
    "            Normalize.from_stats(*imagenet_stats)\n",
    "        ]\n",
    "    ).dataloaders(path_to_data, bs=256)\n",
    "\n",
    "# Load the previously trained model\n",
    "previous_model_path = '/workspace/models/model.pkl'  # replace with your actual model path\n",
    "learn_long = load_learner(previous_model_path) #note that this is creating a different learner than the one we used above\n",
    "\n",
    "# Ensure data loaders are set up\n",
    "learn_long.dls = get_dls(path) # path is the path to our data, which was set earlier\n",
    "\n",
    "# Instantiate the custom callback\n",
    "save_model_callback = SaveModelCallback(every=save_after_this_many_epochs, path='/workspace/models/model')\n",
    "\n",
    "# Define the number of additional epochs you want to train for\n",
    "additional_epochs = 50  # Set the total number of epochs\n",
    "\n",
    "# Continue training the model with the custom callback\n",
    "learn_long.fine_tune(additional_epochs, base_lr=learning_rate, cbs=[save_model_callback])\n",
    "\n",
    "# Save the updated model at the end\n",
    "updated_model_path = f'/workspace/models/updated_model.pkl'\n",
    "learn_long.export(updated_model_path)\n",
    "\n",
    "print(\"Training completed and model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of that training, you may want to take another look at the confusion matrix. How does it compare to your initial training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_long = ClassificationInterpretation.from_learner(learn_long)\n",
    "interp_long.plot_confusion_matrix(normalize=True, title='Confusion Matrix', figsize=(12, 12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
