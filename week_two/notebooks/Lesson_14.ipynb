{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19R3qLLZyAgU"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import requests\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from io import BytesIO\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lakota AI Code Camp Lesson 14: Introduction to Neural Networks IV - Loss Functions and Metrics\n",
        "\n",
        "## Introduction\n",
        "\n",
        "We've seen loss functions and metric in prior lessons, when we saw the neural networks train.\n",
        "Last time it was what was called **Mean Squared Error**, but there are others.\n",
        "\n",
        "We've also seen metrics.\n",
        "For example, when we trained the neural network model yesterday.\n",
        "The metric we were concerned about was the accuracy of our prediction.\n",
        "\n",
        "We'll go more into this later in the lesson.\n",
        "\n",
        "## Loss Functions\n",
        "\n",
        "A loss function is a function that is a proxy for how well our model predicts a label or category.\n",
        "In the context of neural networks, we require it to output a real number.\n",
        "In particular, this is the number we want to reduce by training our neural network.\n",
        "\n",
        "We'll go through some loss functions, their mathematical definition, and how to program them from scratch.\n",
        "\n",
        "### Binary Cross Entropy\n",
        "\n",
        "Binary cross entropy is a loss function that you would use if you were trying to use a model to classify two categories: for example, is this a picture of a dog or cat?\n",
        "\n",
        "The mathematical definition is:\n",
        "$$\n",
        "y \\log \\widehat{y} + (1 - y) \\log (1 - \\widehat{y})\n",
        "$$\n",
        "where $y$ is the actual label and $\\widehat{y}$ is the predicted label.\n",
        "Let's define it from scratch:"
      ],
      "metadata": {
        "id": "vv5kVtkINzeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(actual, predict):\n",
        "\n",
        "    loss = actual * torch.log(predict) + (1 - actual) * torch.log(1 - predict)\n",
        "\n",
        "    return -loss"
      ],
      "metadata": {
        "id": "8IszTDNCs7Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.nn.Sigmoid()(torch.randn(3))\n",
        "target = torch.bernoulli(input)"
      ],
      "metadata": {
        "id": "PJ2MIYQivxb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_cross_entropy(target, input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moKlos8Hv7sf",
        "outputId": "495118f5-edf6-4c51-f85b-20fa5edcbf38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.5331, -0.0360, -0.3462])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.BCELoss(reduction='none')(input, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8iWzWEev8FY",
        "outputId": "97610ed8-de7d-42d8-84d3-016ffb866c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5331, 0.0360, 0.3462])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a possible problem.\n",
        "If our model is very confident in its prediction, either negative or posisitive, then either $\\widehat{y}$ or $1 - \\widehat{y}$ is very close to zero.\n",
        "You may or may not know that the logarithm function has a singularity at 0.\n",
        "Specifically, the closer we get to 0, the larger the loss is.\n",
        "Additionally, our gradient gets larger, which will stop our training.\n",
        "\n",
        "In the current PyTorch version, at this time, 2.0.1, there are only two meaningful parameters for this loss function:\n",
        "1.  `weight`;\n",
        "1.  `reduction`.\n",
        "\n",
        "At most, you will probably only use reduction, but it's probably better to stick with the default parameters.\n"
      ],
      "metadata": {
        "id": "1tbuA84iw_3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross Entropy\n",
        "\n",
        "Cross entropy is the generalization of binary cross entropy loss.\n",
        "We can have arbitrarily many categories as opposed to only two categories.\n",
        "Suppose that we have $n$ categories, then it would be:\n",
        "$$\n",
        "\\sum_{i = 1}^{n} y_{i} \\log \\widehat{y}_{i}.\n",
        "$$\n",
        "where ${\\bf y} = (y_{1}, \\ldots, y_{n})$ is our one-hot encoding and ${\\bf \\widehat{y}} = (\\widehat{y}_{1}, \\ldots, \\widehat{y}_{n})$ is the prediction.\n",
        "\n",
        "In practice, as ${\\bf y}$ will typically have only one-nonzero value, say $j$, then our loss will be $\\log \\widehat{y}_{j}$.\n",
        "\n",
        "This loss function is heavily used in image recognition."
      ],
      "metadata": {
        "id": "tPdLeVAZ0_kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict = F.softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n",
        "actual = torch.empty(3, dtype=torch.long).random_(5)"
      ],
      "metadata": {
        "id": "I0UekGVJ2v-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DzJrY6nv8gHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.CrossEntropyLoss(reduction='none')(predict, actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6c8ZjSC69C8",
        "outputId": "d5516b91-dcc3-453a-bc30-f72415f70ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.7429, 1.7805, 1.6963], grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $L_{1}$ loss\n",
        "\n",
        "The $L_{1}$ loss is generated by what is called a **norm**.\n",
        "The most common norm that you may have seen is the absolute value.\n",
        "Mathematically, this loss is defined as:\n",
        "$$\n",
        "\\sum_{i = 1}^{n} |x_{i} - y_{i}|\n",
        "$$\n",
        "where ${\\bf x} = (x_{1}, \\ldots, x_{n})$ and ${\\bf y} = (y_{1}, \\ldots, y_{n})$ are $n$-dimensional vectors."
      ],
      "metadata": {
        "id": "fzVPwCyX07nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def l1_loss(actual, predict):\n",
        "\n",
        "    loss = torch.abs(actual - predict)\n",
        "    loss = torch.sum(loss)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Lje8mllNv_OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1, 2, 3], dtype=torch.float)\n",
        "y = torch.tensor([0, 1, 2], dtype=torch.float)"
      ],
      "metadata": {
        "id": "5R7u_IIRy5Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1_loss(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDFDzYXo0WpS",
        "outputId": "72d73e0e-7c53-4590-becb-c7513305e517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.L1Loss(reduction='sum')(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BUiI-9c0X4K",
        "outputId": "b88a10b5-c25c-4a81-a029-277a74256719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Squared Error\n",
        "\n",
        "We saw this error last time.\n",
        "It also comes from a norm, but this norm is the usual distance you think of (as the crow flies).\n",
        "\n",
        "Mathematically, this loss is defined as:\n",
        "$$\n",
        "\\sum_{i = 1}^{n} (y_{i} - \\widehat{y}_{i})^{2}\n",
        "$$\n",
        "where ${\\bf y} = (y_{1}, \\ldots, y_{n})$ and ${\\bf \\widehat{y}} = (\\widehat{y}_{1}, \\ldots, \\widehat{y}_{n})$."
      ],
      "metadata": {
        "id": "JrIJzwuB-y9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(actual, predict):\n",
        "\n",
        "    loss = torch.square(actual - predict)\n",
        "    loss = torch.mean(loss)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "agi89dpb0cPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict = torch.randn(3, 5)\n",
        "actual = torch.randn(3, 5)"
      ],
      "metadata": {
        "id": "Ho7Hr7En_aUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_loss(actual, predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhye4uHM_jsP",
        "outputId": "28210473-6969-4912-c541-d9c0ffcf67d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1471)"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.MSELoss()(predict, actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsvgl24J_k6X",
        "outputId": "a8884e65-b2c1-4f31-bd98-b8ce095bdfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1471)"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Detection Loss\n",
        "\n",
        "We'll talk more about this in a later lecture, due to the complexity of the loss and metrics."
      ],
      "metadata": {
        "id": "mtFt488IBTd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "\n",
        "## Accuracy\n",
        "\n",
        "One of the most common metrics is accuracy.\n",
        "It's basically the number of correct predictions divided by the total number of predictions.\n",
        "This metric is particularly useful with image recognition models.\n",
        "\n",
        "Let's look at code from the last lecture:"
      ],
      "metadata": {
        "id": "gdgDU229IHsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, label in test_loader:\n",
        "            inputs, label = inputs.to(device), label.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on 10000 test images: {100 * correct // total} %')\n",
        "    return 100 * correct // total"
      ],
      "metadata": {
        "id": "qyIU59AA_r50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Area Under ROC curve (AUROC)\n",
        "\n",
        "Before we talk about AUROC, we'll make a short digression into the different ways a model can classify outputs.\n",
        "Further, we're going to assume that we're talking about binary models only.\n",
        "The **true positive rate** is the number of actual positive results classified as positive divided by the total number of positives.\n",
        "The **true negative rate** is similar.\n",
        "Its the number of actual negative results classified as negative divided by the total negatives.\n",
        "\n",
        "The **false positive rate** is the number of negatives that were incorrectly classified by positive as our model divided by the total negatives.\n",
        "The **false negatives rate** is similar.\n",
        "It is hte number of positives that were incorrectly classified as negative, divided by the total number of negatives.\n",
        "\n",
        "The ROC (receiver operating characteristics) curve is graphed with the $y$-axis as the true positive rate and the $x$-axis as the false positive rate.\n",
        "\n",
        "Models are supposed to give predictions, but given an input our model spits out a number.\n",
        "It's up to us to determine how to use that number.\n",
        "That's where thresholding comes in.\n",
        "We plot the true negative rate vs the false positive rate for various thresholds.\n",
        "\n",
        "So, we can say that anything above a $0.5$ will be classified as a positive example.\n",
        "We can replace $0.5$ with any number between 0 and 1.\n",
        "Then we can graph the ROC curve.\n",
        "\n",
        "Finally, the area under the ROC curve is its name.\n",
        "\n",
        "We won't use it, but it's an important metric in data science, so you should try to remember it."
      ],
      "metadata": {
        "id": "1O79Stp6Idc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IoU\n",
        "\n",
        "We'll briefly go over the IoU metric.\n",
        "IoU stands for Intersection over Union metric.\n",
        "Given two bounding boxes, we calculate the area of their intersection and the artheir union.\n",
        "The IoU of two bounding boxes is the intersection divided by the union.\n",
        "We'll go over this more in a future lecture.\n"
      ],
      "metadata": {
        "id": "Eu1e_5KoL1z_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xa-ngNvoIcja"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}