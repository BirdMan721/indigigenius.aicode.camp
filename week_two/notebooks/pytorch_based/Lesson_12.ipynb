{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nja5yRhH0LnF"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import requests\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from scipy.special import erf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lakota AI Code Camp Lesson 12: Introduction to Neural Networks II - Layers\n",
        "\n",
        "We're going to talk about the layers in this section.\n",
        "First, we'll introduce linear layers and then we'll introduce convolutional layers.\n",
        "Next, we'll compare a linear neural network and a convolutional linear neural network.\n",
        "Then tomorrow, we're going to build LeNet.\n",
        "This paper was first implemented in the paper *Backpropagation Applied to Handwritten Zip Code Recognition*, developed at **AT&T Bell Laboratories** by\n",
        "\n",
        "*   Y. LeCun\n",
        "*   B. Boser\n",
        "*   J. S. Denker\n",
        "*   D. Henderson\n",
        "*   R. E. Howard\n",
        "*   W. Hubbard\n",
        "*   L. D. Jackel\n",
        "\n",
        "## Layers\n",
        "\n",
        "Layers approximate the neurons that inspired the neural network.\n",
        "Each neuron receives a signal from other neurons and then those signals may or may not cause the neuron to send off another signal, depending on the inputs.\n",
        "There is a lot more going on within the neurons, but this is what we're trying to approximate.\n",
        "\n",
        "### Linear Layers\n",
        "\n",
        "We've talked about matrix algebra earlier and the amazing thing is that this gives us enough mathematics to develop a **simple** model of the neurons in a brain.\n",
        "We introduced the matrix multiplication and vector addition and this is what gives us our first type of neural network layer.\n",
        "If our input ${\\bf x}$ is an $n$-dimensional vector, then if we have an $m \\times n$ matrix ${\\bf W}$ and an $n$-dimensional vector ${\\bf b}$, then our layer is:\n",
        "$$\n",
        "{\\bf W} {\\bf x} + {\\bf b}.\n",
        "$$\n",
        "The matrix ${\\bf W}$ is often called the **weight** matrix of our layer and the vector ${\\bf b}$ is called the **bias**.\n",
        "\n",
        "Let's build a two layer neural network to classify digits!"
      ],
      "metadata": {
        "id": "AbgA3zTG0P1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(28 * 28, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(256, 10),\n",
        "    torch.nn.Softmax(dim=-1)\n",
        ")"
      ],
      "metadata": {
        "id": "9rTgpFuG-ldE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.zeros(28 * 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-KjlHat9yvn",
        "outputId": "424f794e-1050-4f50-cbea-a8676732ca1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0966, 0.1056, 0.1016, 0.0971, 0.0950, 0.1035, 0.0936, 0.0993, 0.1051,\n",
              "        0.1026], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we have to get the MNIST dataset."
      ],
      "metadata": {
        "id": "vtcSzRiY_uKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize(mean=torch.tensor([0.1307]), std=torch.tensor([0.3081])),\n",
        "    ])"
      ],
      "metadata": {
        "id": "vkUazpsrAEnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = torchvision.datasets.MNIST(root='./data',\n",
        "                                      train=True,\n",
        "                                      download=True,\n",
        "                                      transform=transforms)\n",
        "\n",
        "test_ds = torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     download=True,\n",
        "                                     transform=transforms)"
      ],
      "metadata": {
        "id": "mrRrZ9eX_t47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = torch.utils.data.DataLoader(train_ds,\n",
        "                                       batch_size=len(train_ds) // 16,\n",
        "                                       shuffle=True,\n",
        "                                       num_workers=0)\n",
        "test_dl = torch.utils.data.DataLoader(test_ds,\n",
        "                                      batch_size=len(test_ds),\n",
        "                                      shuffle=False,\n",
        "                                      num_workers=0)"
      ],
      "metadata": {
        "id": "XrT9LmTy__d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)"
      ],
      "metadata": {
        "id": "0r9tPsxIAjsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_ZbJ1RfAvud",
        "outputId": "8a6907ea-6b34-4687-c73a-0c4472774506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
              "  (3): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, label in train_dl:\n",
        "    print(inputs.shape, label.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eupcYN4Bpwv",
        "outputId": "aa2a9aca-84bd-469e-dc9f-684df5b253dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3750, 1, 28, 28]) torch.Size([3750])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(23):\n",
        "    for batch, (inputs, label) in enumerate(train_dl):\n",
        "        inputs, label = inputs.to(device), label.to(device)\n",
        "\n",
        "        inputs = torch.flatten(inputs, start_dim=-3)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        out = model(inputs)\n",
        "        loss = criterion(out, label)\n",
        "        print(f\"Epoch: {epoch + 1}; Batch: {batch + 1}; Loss: {loss}\")\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    print(f\"Finished epoch {epoch + 1}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0bKGtmc_nkS",
        "outputId": "cba89ed5-1233-4f1f-b08a-6ba07f430bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1; Batch: 1; Loss: 2.3019461631774902\n",
            "Epoch: 1; Batch: 2; Loss: 2.2997820377349854\n",
            "Epoch: 1; Batch: 3; Loss: 2.2967216968536377\n",
            "Epoch: 1; Batch: 4; Loss: 2.291823625564575\n",
            "Epoch: 1; Batch: 5; Loss: 2.2850635051727295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fb68a7229e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 132, in __del__\n",
            "    self._close()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
            "    _close(self._handle)\n",
            "OSError: [Errno 9] Bad file descriptor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1; Batch: 6; Loss: 2.276172399520874\n",
            "Epoch: 1; Batch: 7; Loss: 2.2651891708374023\n",
            "Epoch: 1; Batch: 8; Loss: 2.248570203781128\n",
            "Epoch: 1; Batch: 9; Loss: 2.22991681098938\n",
            "Epoch: 1; Batch: 10; Loss: 2.2025718688964844\n",
            "Epoch: 1; Batch: 11; Loss: 2.162245750427246\n",
            "Epoch: 1; Batch: 12; Loss: 2.1160595417022705\n",
            "Epoch: 1; Batch: 13; Loss: 2.0580484867095947\n",
            "Epoch: 1; Batch: 14; Loss: 1.9979041814804077\n",
            "Epoch: 1; Batch: 15; Loss: 1.9451444149017334\n",
            "Epoch: 1; Batch: 16; Loss: 1.8914061784744263\n",
            "Finished epoch 1.\n",
            "Epoch: 2; Batch: 1; Loss: 1.854122519493103\n",
            "Epoch: 2; Batch: 2; Loss: 1.8224453926086426\n",
            "Epoch: 2; Batch: 3; Loss: 1.7998038530349731\n",
            "Epoch: 2; Batch: 4; Loss: 1.7781126499176025\n",
            "Epoch: 2; Batch: 5; Loss: 1.771030306816101\n",
            "Epoch: 2; Batch: 6; Loss: 1.7672470808029175\n",
            "Epoch: 2; Batch: 7; Loss: 1.7641061544418335\n",
            "Epoch: 2; Batch: 8; Loss: 1.7544360160827637\n",
            "Epoch: 2; Batch: 9; Loss: 1.7508050203323364\n",
            "Epoch: 2; Batch: 10; Loss: 1.7367897033691406\n",
            "Epoch: 2; Batch: 11; Loss: 1.7479249238967896\n",
            "Epoch: 2; Batch: 12; Loss: 1.7373992204666138\n",
            "Epoch: 2; Batch: 13; Loss: 1.7353934049606323\n",
            "Epoch: 2; Batch: 14; Loss: 1.7403100728988647\n",
            "Epoch: 2; Batch: 15; Loss: 1.7352780103683472\n",
            "Epoch: 2; Batch: 16; Loss: 1.737252950668335\n",
            "Finished epoch 2.\n",
            "Epoch: 3; Batch: 1; Loss: 1.7183035612106323\n",
            "Epoch: 3; Batch: 2; Loss: 1.7238659858703613\n",
            "Epoch: 3; Batch: 3; Loss: 1.7254137992858887\n",
            "Epoch: 3; Batch: 4; Loss: 1.718163251876831\n",
            "Epoch: 3; Batch: 5; Loss: 1.7170677185058594\n",
            "Epoch: 3; Batch: 6; Loss: 1.722092628479004\n",
            "Epoch: 3; Batch: 7; Loss: 1.721707820892334\n",
            "Epoch: 3; Batch: 8; Loss: 1.7177385091781616\n",
            "Epoch: 3; Batch: 9; Loss: 1.7112228870391846\n",
            "Epoch: 3; Batch: 10; Loss: 1.7213140726089478\n",
            "Epoch: 3; Batch: 11; Loss: 1.7194844484329224\n",
            "Epoch: 3; Batch: 12; Loss: 1.7148323059082031\n",
            "Epoch: 3; Batch: 13; Loss: 1.7165504693984985\n",
            "Epoch: 3; Batch: 14; Loss: 1.7051939964294434\n",
            "Epoch: 3; Batch: 15; Loss: 1.7052780389785767\n",
            "Epoch: 3; Batch: 16; Loss: 1.6852484941482544\n",
            "Finished epoch 3.\n",
            "Epoch: 4; Batch: 1; Loss: 1.687220573425293\n",
            "Epoch: 4; Batch: 2; Loss: 1.6691699028015137\n",
            "Epoch: 4; Batch: 3; Loss: 1.6972817182540894\n",
            "Epoch: 4; Batch: 4; Loss: 1.6830880641937256\n",
            "Epoch: 4; Batch: 5; Loss: 1.6807143688201904\n",
            "Epoch: 4; Batch: 6; Loss: 1.6819840669631958\n",
            "Epoch: 4; Batch: 7; Loss: 1.6736993789672852\n",
            "Epoch: 4; Batch: 8; Loss: 1.6668747663497925\n",
            "Epoch: 4; Batch: 9; Loss: 1.6625046730041504\n",
            "Epoch: 4; Batch: 10; Loss: 1.660640835762024\n",
            "Epoch: 4; Batch: 11; Loss: 1.6621342897415161\n",
            "Epoch: 4; Batch: 12; Loss: 1.6682852506637573\n",
            "Epoch: 4; Batch: 13; Loss: 1.6531245708465576\n",
            "Epoch: 4; Batch: 14; Loss: 1.6658395528793335\n",
            "Epoch: 4; Batch: 15; Loss: 1.6604832410812378\n",
            "Epoch: 4; Batch: 16; Loss: 1.649693250656128\n",
            "Finished epoch 4.\n",
            "Epoch: 5; Batch: 1; Loss: 1.6599324941635132\n",
            "Epoch: 5; Batch: 2; Loss: 1.6413582563400269\n",
            "Epoch: 5; Batch: 3; Loss: 1.632338285446167\n",
            "Epoch: 5; Batch: 4; Loss: 1.6013460159301758\n",
            "Epoch: 5; Batch: 5; Loss: 1.6071830987930298\n",
            "Epoch: 5; Batch: 6; Loss: 1.6199678182601929\n",
            "Epoch: 5; Batch: 7; Loss: 1.6204936504364014\n",
            "Epoch: 5; Batch: 8; Loss: 1.6017351150512695\n",
            "Epoch: 5; Batch: 9; Loss: 1.597959041595459\n",
            "Epoch: 5; Batch: 10; Loss: 1.6097478866577148\n",
            "Epoch: 5; Batch: 11; Loss: 1.5997273921966553\n",
            "Epoch: 5; Batch: 12; Loss: 1.585798978805542\n",
            "Epoch: 5; Batch: 13; Loss: 1.5877338647842407\n",
            "Epoch: 5; Batch: 14; Loss: 1.5933805704116821\n",
            "Epoch: 5; Batch: 15; Loss: 1.589263916015625\n",
            "Epoch: 5; Batch: 16; Loss: 1.5714406967163086\n",
            "Finished epoch 5.\n",
            "Epoch: 6; Batch: 1; Loss: 1.5740710496902466\n",
            "Epoch: 6; Batch: 2; Loss: 1.5853937864303589\n",
            "Epoch: 6; Batch: 3; Loss: 1.574224591255188\n",
            "Epoch: 6; Batch: 4; Loss: 1.5730268955230713\n",
            "Epoch: 6; Batch: 5; Loss: 1.5680615901947021\n",
            "Epoch: 6; Batch: 6; Loss: 1.5764919519424438\n",
            "Epoch: 6; Batch: 7; Loss: 1.5774134397506714\n",
            "Epoch: 6; Batch: 8; Loss: 1.5677993297576904\n",
            "Epoch: 6; Batch: 9; Loss: 1.57244873046875\n",
            "Epoch: 6; Batch: 10; Loss: 1.5723826885223389\n",
            "Epoch: 6; Batch: 11; Loss: 1.574427843093872\n",
            "Epoch: 6; Batch: 12; Loss: 1.573891043663025\n",
            "Epoch: 6; Batch: 13; Loss: 1.5715689659118652\n",
            "Epoch: 6; Batch: 14; Loss: 1.5712711811065674\n",
            "Epoch: 6; Batch: 15; Loss: 1.5638591051101685\n",
            "Epoch: 6; Batch: 16; Loss: 1.560144066810608\n",
            "Finished epoch 6.\n",
            "Epoch: 7; Batch: 1; Loss: 1.5633141994476318\n",
            "Epoch: 7; Batch: 2; Loss: 1.5736273527145386\n",
            "Epoch: 7; Batch: 3; Loss: 1.563538908958435\n",
            "Epoch: 7; Batch: 4; Loss: 1.5623266696929932\n",
            "Epoch: 7; Batch: 5; Loss: 1.558432698249817\n",
            "Epoch: 7; Batch: 6; Loss: 1.5607237815856934\n",
            "Epoch: 7; Batch: 7; Loss: 1.564589500427246\n",
            "Epoch: 7; Batch: 8; Loss: 1.5623631477355957\n",
            "Epoch: 7; Batch: 9; Loss: 1.5590453147888184\n",
            "Epoch: 7; Batch: 10; Loss: 1.5579622983932495\n",
            "Epoch: 7; Batch: 11; Loss: 1.5556268692016602\n",
            "Epoch: 7; Batch: 12; Loss: 1.5523340702056885\n",
            "Epoch: 7; Batch: 13; Loss: 1.5624812841415405\n",
            "Epoch: 7; Batch: 14; Loss: 1.556026577949524\n",
            "Epoch: 7; Batch: 15; Loss: 1.5561788082122803\n",
            "Epoch: 7; Batch: 16; Loss: 1.5531665086746216\n",
            "Finished epoch 7.\n",
            "Epoch: 8; Batch: 1; Loss: 1.5615826845169067\n",
            "Epoch: 8; Batch: 2; Loss: 1.555032730102539\n",
            "Epoch: 8; Batch: 3; Loss: 1.5479333400726318\n",
            "Epoch: 8; Batch: 4; Loss: 1.5528531074523926\n",
            "Epoch: 8; Batch: 5; Loss: 1.5558708906173706\n",
            "Epoch: 8; Batch: 6; Loss: 1.5509064197540283\n",
            "Epoch: 8; Batch: 7; Loss: 1.554641842842102\n",
            "Epoch: 8; Batch: 8; Loss: 1.5544726848602295\n",
            "Epoch: 8; Batch: 9; Loss: 1.5503802299499512\n",
            "Epoch: 8; Batch: 10; Loss: 1.5538605451583862\n",
            "Epoch: 8; Batch: 11; Loss: 1.5473166704177856\n",
            "Epoch: 8; Batch: 12; Loss: 1.5577718019485474\n",
            "Epoch: 8; Batch: 13; Loss: 1.5600835084915161\n",
            "Epoch: 8; Batch: 14; Loss: 1.5522892475128174\n",
            "Epoch: 8; Batch: 15; Loss: 1.5482975244522095\n",
            "Epoch: 8; Batch: 16; Loss: 1.5488015413284302\n",
            "Finished epoch 8.\n",
            "Epoch: 9; Batch: 1; Loss: 1.5502594709396362\n",
            "Epoch: 9; Batch: 2; Loss: 1.5494294166564941\n",
            "Epoch: 9; Batch: 3; Loss: 1.548076868057251\n",
            "Epoch: 9; Batch: 4; Loss: 1.557769536972046\n",
            "Epoch: 9; Batch: 5; Loss: 1.5490583181381226\n",
            "Epoch: 9; Batch: 6; Loss: 1.5521138906478882\n",
            "Epoch: 9; Batch: 7; Loss: 1.541450023651123\n",
            "Epoch: 9; Batch: 8; Loss: 1.551092505455017\n",
            "Epoch: 9; Batch: 9; Loss: 1.5449899435043335\n",
            "Epoch: 9; Batch: 10; Loss: 1.5474518537521362\n",
            "Epoch: 9; Batch: 11; Loss: 1.5423099994659424\n",
            "Epoch: 9; Batch: 12; Loss: 1.5475913286209106\n",
            "Epoch: 9; Batch: 13; Loss: 1.5502034425735474\n",
            "Epoch: 9; Batch: 14; Loss: 1.5498653650283813\n",
            "Epoch: 9; Batch: 15; Loss: 1.5466556549072266\n",
            "Epoch: 9; Batch: 16; Loss: 1.5500329732894897\n",
            "Finished epoch 9.\n",
            "Epoch: 10; Batch: 1; Loss: 1.5545806884765625\n",
            "Epoch: 10; Batch: 2; Loss: 1.5542012453079224\n",
            "Epoch: 10; Batch: 3; Loss: 1.5474517345428467\n",
            "Epoch: 10; Batch: 4; Loss: 1.5432816743850708\n",
            "Epoch: 10; Batch: 5; Loss: 1.5433050394058228\n",
            "Epoch: 10; Batch: 6; Loss: 1.540805697441101\n",
            "Epoch: 10; Batch: 7; Loss: 1.5471957921981812\n",
            "Epoch: 10; Batch: 8; Loss: 1.5446486473083496\n",
            "Epoch: 10; Batch: 9; Loss: 1.5392485857009888\n",
            "Epoch: 10; Batch: 10; Loss: 1.5415465831756592\n",
            "Epoch: 10; Batch: 11; Loss: 1.544540286064148\n",
            "Epoch: 10; Batch: 12; Loss: 1.546905755996704\n",
            "Epoch: 10; Batch: 13; Loss: 1.543241024017334\n",
            "Epoch: 10; Batch: 14; Loss: 1.542769432067871\n",
            "Epoch: 10; Batch: 15; Loss: 1.536816120147705\n",
            "Epoch: 10; Batch: 16; Loss: 1.5446815490722656\n",
            "Finished epoch 10.\n",
            "Epoch: 11; Batch: 1; Loss: 1.540523648262024\n",
            "Epoch: 11; Batch: 2; Loss: 1.5409715175628662\n",
            "Epoch: 11; Batch: 3; Loss: 1.5398203134536743\n",
            "Epoch: 11; Batch: 4; Loss: 1.536027431488037\n",
            "Epoch: 11; Batch: 5; Loss: 1.5431278944015503\n",
            "Epoch: 11; Batch: 6; Loss: 1.5403767824172974\n",
            "Epoch: 11; Batch: 7; Loss: 1.541254997253418\n",
            "Epoch: 11; Batch: 8; Loss: 1.5433112382888794\n",
            "Epoch: 11; Batch: 9; Loss: 1.5442326068878174\n",
            "Epoch: 11; Batch: 10; Loss: 1.5457813739776611\n",
            "Epoch: 11; Batch: 11; Loss: 1.542702317237854\n",
            "Epoch: 11; Batch: 12; Loss: 1.5406218767166138\n",
            "Epoch: 11; Batch: 13; Loss: 1.5395736694335938\n",
            "Epoch: 11; Batch: 14; Loss: 1.5455867052078247\n",
            "Epoch: 11; Batch: 15; Loss: 1.533390998840332\n",
            "Epoch: 11; Batch: 16; Loss: 1.5435435771942139\n",
            "Finished epoch 11.\n",
            "Epoch: 12; Batch: 1; Loss: 1.5371571779251099\n",
            "Epoch: 12; Batch: 2; Loss: 1.5422905683517456\n",
            "Epoch: 12; Batch: 3; Loss: 1.539638638496399\n",
            "Epoch: 12; Batch: 4; Loss: 1.5394001007080078\n",
            "Epoch: 12; Batch: 5; Loss: 1.540177583694458\n",
            "Epoch: 12; Batch: 6; Loss: 1.5382158756256104\n",
            "Epoch: 12; Batch: 7; Loss: 1.5307954549789429\n",
            "Epoch: 12; Batch: 8; Loss: 1.5365424156188965\n",
            "Epoch: 12; Batch: 9; Loss: 1.5376513004302979\n",
            "Epoch: 12; Batch: 10; Loss: 1.5417225360870361\n",
            "Epoch: 12; Batch: 11; Loss: 1.5368831157684326\n",
            "Epoch: 12; Batch: 12; Loss: 1.5387839078903198\n",
            "Epoch: 12; Batch: 13; Loss: 1.5421398878097534\n",
            "Epoch: 12; Batch: 14; Loss: 1.5385618209838867\n",
            "Epoch: 12; Batch: 15; Loss: 1.5349783897399902\n",
            "Epoch: 12; Batch: 16; Loss: 1.5388821363449097\n",
            "Finished epoch 12.\n",
            "Epoch: 13; Batch: 1; Loss: 1.530216932296753\n",
            "Epoch: 13; Batch: 2; Loss: 1.5331616401672363\n",
            "Epoch: 13; Batch: 3; Loss: 1.532535433769226\n",
            "Epoch: 13; Batch: 4; Loss: 1.5335254669189453\n",
            "Epoch: 13; Batch: 5; Loss: 1.5419065952301025\n",
            "Epoch: 13; Batch: 6; Loss: 1.5331138372421265\n",
            "Epoch: 13; Batch: 7; Loss: 1.539760708808899\n",
            "Epoch: 13; Batch: 8; Loss: 1.5404107570648193\n",
            "Epoch: 13; Batch: 9; Loss: 1.5296130180358887\n",
            "Epoch: 13; Batch: 10; Loss: 1.5368977785110474\n",
            "Epoch: 13; Batch: 11; Loss: 1.5377668142318726\n",
            "Epoch: 13; Batch: 12; Loss: 1.5351073741912842\n",
            "Epoch: 13; Batch: 13; Loss: 1.530859351158142\n",
            "Epoch: 13; Batch: 14; Loss: 1.5352802276611328\n",
            "Epoch: 13; Batch: 15; Loss: 1.5431644916534424\n",
            "Epoch: 13; Batch: 16; Loss: 1.537501335144043\n",
            "Finished epoch 13.\n",
            "Epoch: 14; Batch: 1; Loss: 1.5338295698165894\n",
            "Epoch: 14; Batch: 2; Loss: 1.5274325609207153\n",
            "Epoch: 14; Batch: 3; Loss: 1.526814579963684\n",
            "Epoch: 14; Batch: 4; Loss: 1.5366933345794678\n",
            "Epoch: 14; Batch: 5; Loss: 1.5347387790679932\n",
            "Epoch: 14; Batch: 6; Loss: 1.5377378463745117\n",
            "Epoch: 14; Batch: 7; Loss: 1.5396391153335571\n",
            "Epoch: 14; Batch: 8; Loss: 1.5369892120361328\n",
            "Epoch: 14; Batch: 9; Loss: 1.525875449180603\n",
            "Epoch: 14; Batch: 10; Loss: 1.537157654762268\n",
            "Epoch: 14; Batch: 11; Loss: 1.5360833406448364\n",
            "Epoch: 14; Batch: 12; Loss: 1.5340657234191895\n",
            "Epoch: 14; Batch: 13; Loss: 1.5322411060333252\n",
            "Epoch: 14; Batch: 14; Loss: 1.531632900238037\n",
            "Epoch: 14; Batch: 15; Loss: 1.5256414413452148\n",
            "Epoch: 14; Batch: 16; Loss: 1.5336359739303589\n",
            "Finished epoch 14.\n",
            "Epoch: 15; Batch: 1; Loss: 1.530625343322754\n",
            "Epoch: 15; Batch: 2; Loss: 1.5336437225341797\n",
            "Epoch: 15; Batch: 3; Loss: 1.5318694114685059\n",
            "Epoch: 15; Batch: 4; Loss: 1.5342155694961548\n",
            "Epoch: 15; Batch: 5; Loss: 1.53437340259552\n",
            "Epoch: 15; Batch: 6; Loss: 1.526214599609375\n",
            "Epoch: 15; Batch: 7; Loss: 1.5328558683395386\n",
            "Epoch: 15; Batch: 8; Loss: 1.5346460342407227\n",
            "Epoch: 15; Batch: 9; Loss: 1.5364880561828613\n",
            "Epoch: 15; Batch: 10; Loss: 1.5259439945220947\n",
            "Epoch: 15; Batch: 11; Loss: 1.5239274501800537\n",
            "Epoch: 15; Batch: 12; Loss: 1.5316461324691772\n",
            "Epoch: 15; Batch: 13; Loss: 1.5256221294403076\n",
            "Epoch: 15; Batch: 14; Loss: 1.531603217124939\n",
            "Epoch: 15; Batch: 15; Loss: 1.5301748514175415\n",
            "Epoch: 15; Batch: 16; Loss: 1.5306181907653809\n",
            "Finished epoch 15.\n",
            "Epoch: 16; Batch: 1; Loss: 1.526146411895752\n",
            "Epoch: 16; Batch: 2; Loss: 1.5285528898239136\n",
            "Epoch: 16; Batch: 3; Loss: 1.5263882875442505\n",
            "Epoch: 16; Batch: 4; Loss: 1.5234122276306152\n",
            "Epoch: 16; Batch: 5; Loss: 1.5315698385238647\n",
            "Epoch: 16; Batch: 6; Loss: 1.5341895818710327\n",
            "Epoch: 16; Batch: 7; Loss: 1.5320920944213867\n",
            "Epoch: 16; Batch: 8; Loss: 1.5311470031738281\n",
            "Epoch: 16; Batch: 9; Loss: 1.537753939628601\n",
            "Epoch: 16; Batch: 10; Loss: 1.5289294719696045\n",
            "Epoch: 16; Batch: 11; Loss: 1.5266168117523193\n",
            "Epoch: 16; Batch: 12; Loss: 1.5267225503921509\n",
            "Epoch: 16; Batch: 13; Loss: 1.5315542221069336\n",
            "Epoch: 16; Batch: 14; Loss: 1.5251120328903198\n",
            "Epoch: 16; Batch: 15; Loss: 1.522120475769043\n",
            "Epoch: 16; Batch: 16; Loss: 1.5295778512954712\n",
            "Finished epoch 16.\n",
            "Epoch: 17; Batch: 1; Loss: 1.5312926769256592\n",
            "Epoch: 17; Batch: 2; Loss: 1.5297657251358032\n",
            "Epoch: 17; Batch: 3; Loss: 1.5207496881484985\n",
            "Epoch: 17; Batch: 4; Loss: 1.5284979343414307\n",
            "Epoch: 17; Batch: 5; Loss: 1.5272923707962036\n",
            "Epoch: 17; Batch: 6; Loss: 1.5269852876663208\n",
            "Epoch: 17; Batch: 7; Loss: 1.523040771484375\n",
            "Epoch: 17; Batch: 8; Loss: 1.5297995805740356\n",
            "Epoch: 17; Batch: 9; Loss: 1.5283547639846802\n",
            "Epoch: 17; Batch: 10; Loss: 1.5261567831039429\n",
            "Epoch: 17; Batch: 11; Loss: 1.5269317626953125\n",
            "Epoch: 17; Batch: 12; Loss: 1.527956247329712\n",
            "Epoch: 17; Batch: 13; Loss: 1.5308035612106323\n",
            "Epoch: 17; Batch: 14; Loss: 1.5221158266067505\n",
            "Epoch: 17; Batch: 15; Loss: 1.5288469791412354\n",
            "Epoch: 17; Batch: 16; Loss: 1.5237044095993042\n",
            "Finished epoch 17.\n",
            "Epoch: 18; Batch: 1; Loss: 1.5191196203231812\n",
            "Epoch: 18; Batch: 2; Loss: 1.527237057685852\n",
            "Epoch: 18; Batch: 3; Loss: 1.523686408996582\n",
            "Epoch: 18; Batch: 4; Loss: 1.5235753059387207\n",
            "Epoch: 18; Batch: 5; Loss: 1.5283608436584473\n",
            "Epoch: 18; Batch: 6; Loss: 1.5237231254577637\n",
            "Epoch: 18; Batch: 7; Loss: 1.524733066558838\n",
            "Epoch: 18; Batch: 8; Loss: 1.5278517007827759\n",
            "Epoch: 18; Batch: 9; Loss: 1.5262744426727295\n",
            "Epoch: 18; Batch: 10; Loss: 1.52324378490448\n",
            "Epoch: 18; Batch: 11; Loss: 1.5288610458374023\n",
            "Epoch: 18; Batch: 12; Loss: 1.5293798446655273\n",
            "Epoch: 18; Batch: 13; Loss: 1.5227406024932861\n",
            "Epoch: 18; Batch: 14; Loss: 1.5237194299697876\n",
            "Epoch: 18; Batch: 15; Loss: 1.5303772687911987\n",
            "Epoch: 18; Batch: 16; Loss: 1.5167256593704224\n",
            "Finished epoch 18.\n",
            "Epoch: 19; Batch: 1; Loss: 1.5250581502914429\n",
            "Epoch: 19; Batch: 2; Loss: 1.5208591222763062\n",
            "Epoch: 19; Batch: 3; Loss: 1.5236098766326904\n",
            "Epoch: 19; Batch: 4; Loss: 1.519614577293396\n",
            "Epoch: 19; Batch: 5; Loss: 1.5271636247634888\n",
            "Epoch: 19; Batch: 6; Loss: 1.5219792127609253\n",
            "Epoch: 19; Batch: 7; Loss: 1.5227372646331787\n",
            "Epoch: 19; Batch: 8; Loss: 1.5239875316619873\n",
            "Epoch: 19; Batch: 9; Loss: 1.5217480659484863\n",
            "Epoch: 19; Batch: 10; Loss: 1.520308256149292\n",
            "Epoch: 19; Batch: 11; Loss: 1.5256061553955078\n",
            "Epoch: 19; Batch: 12; Loss: 1.5272109508514404\n",
            "Epoch: 19; Batch: 13; Loss: 1.5210883617401123\n",
            "Epoch: 19; Batch: 14; Loss: 1.5273298025131226\n",
            "Epoch: 19; Batch: 15; Loss: 1.523666501045227\n",
            "Epoch: 19; Batch: 16; Loss: 1.520530343055725\n",
            "Finished epoch 19.\n",
            "Epoch: 20; Batch: 1; Loss: 1.5170819759368896\n",
            "Epoch: 20; Batch: 2; Loss: 1.5236999988555908\n",
            "Epoch: 20; Batch: 3; Loss: 1.5198312997817993\n",
            "Epoch: 20; Batch: 4; Loss: 1.526007890701294\n",
            "Epoch: 20; Batch: 5; Loss: 1.5231690406799316\n",
            "Epoch: 20; Batch: 6; Loss: 1.5250307321548462\n",
            "Epoch: 20; Batch: 7; Loss: 1.521530032157898\n",
            "Epoch: 20; Batch: 8; Loss: 1.5209804773330688\n",
            "Epoch: 20; Batch: 9; Loss: 1.523557424545288\n",
            "Epoch: 20; Batch: 10; Loss: 1.5184049606323242\n",
            "Epoch: 20; Batch: 11; Loss: 1.5259977579116821\n",
            "Epoch: 20; Batch: 12; Loss: 1.5249336957931519\n",
            "Epoch: 20; Batch: 13; Loss: 1.5167628526687622\n",
            "Epoch: 20; Batch: 14; Loss: 1.517980933189392\n",
            "Epoch: 20; Batch: 15; Loss: 1.5214835405349731\n",
            "Epoch: 20; Batch: 16; Loss: 1.5217905044555664\n",
            "Finished epoch 20.\n",
            "Epoch: 21; Batch: 1; Loss: 1.5177980661392212\n",
            "Epoch: 21; Batch: 2; Loss: 1.5266225337982178\n",
            "Epoch: 21; Batch: 3; Loss: 1.5279113054275513\n",
            "Epoch: 21; Batch: 4; Loss: 1.5171304941177368\n",
            "Epoch: 21; Batch: 5; Loss: 1.5211588144302368\n",
            "Epoch: 21; Batch: 6; Loss: 1.5153511762619019\n",
            "Epoch: 21; Batch: 7; Loss: 1.5191305875778198\n",
            "Epoch: 21; Batch: 8; Loss: 1.518221139907837\n",
            "Epoch: 21; Batch: 9; Loss: 1.524670124053955\n",
            "Epoch: 21; Batch: 10; Loss: 1.5225955247879028\n",
            "Epoch: 21; Batch: 11; Loss: 1.5157221555709839\n",
            "Epoch: 21; Batch: 12; Loss: 1.5223658084869385\n",
            "Epoch: 21; Batch: 13; Loss: 1.5205233097076416\n",
            "Epoch: 21; Batch: 14; Loss: 1.5177440643310547\n",
            "Epoch: 21; Batch: 15; Loss: 1.5175024271011353\n",
            "Epoch: 21; Batch: 16; Loss: 1.5193935632705688\n",
            "Finished epoch 21.\n",
            "Epoch: 22; Batch: 1; Loss: 1.517451286315918\n",
            "Epoch: 22; Batch: 2; Loss: 1.5202864408493042\n",
            "Epoch: 22; Batch: 3; Loss: 1.51080322265625\n",
            "Epoch: 22; Batch: 4; Loss: 1.5200453996658325\n",
            "Epoch: 22; Batch: 5; Loss: 1.5190502405166626\n",
            "Epoch: 22; Batch: 6; Loss: 1.5210403203964233\n",
            "Epoch: 22; Batch: 7; Loss: 1.517292857170105\n",
            "Epoch: 22; Batch: 8; Loss: 1.5212090015411377\n",
            "Epoch: 22; Batch: 9; Loss: 1.517314076423645\n",
            "Epoch: 22; Batch: 10; Loss: 1.51713228225708\n",
            "Epoch: 22; Batch: 11; Loss: 1.5194469690322876\n",
            "Epoch: 22; Batch: 12; Loss: 1.5193310976028442\n",
            "Epoch: 22; Batch: 13; Loss: 1.5179628133773804\n",
            "Epoch: 22; Batch: 14; Loss: 1.5252028703689575\n",
            "Epoch: 22; Batch: 15; Loss: 1.5189043283462524\n",
            "Epoch: 22; Batch: 16; Loss: 1.5188360214233398\n",
            "Finished epoch 22.\n",
            "Epoch: 23; Batch: 1; Loss: 1.520285725593567\n",
            "Epoch: 23; Batch: 2; Loss: 1.5220433473587036\n",
            "Epoch: 23; Batch: 3; Loss: 1.5188497304916382\n",
            "Epoch: 23; Batch: 4; Loss: 1.5182831287384033\n",
            "Epoch: 23; Batch: 5; Loss: 1.5137161016464233\n",
            "Epoch: 23; Batch: 6; Loss: 1.5149532556533813\n",
            "Epoch: 23; Batch: 7; Loss: 1.5195274353027344\n",
            "Epoch: 23; Batch: 8; Loss: 1.5192707777023315\n",
            "Epoch: 23; Batch: 9; Loss: 1.5207897424697876\n",
            "Epoch: 23; Batch: 10; Loss: 1.515713095664978\n",
            "Epoch: 23; Batch: 11; Loss: 1.51686692237854\n",
            "Epoch: 23; Batch: 12; Loss: 1.5124248266220093\n",
            "Epoch: 23; Batch: 13; Loss: 1.5163911581039429\n",
            "Epoch: 23; Batch: 14; Loss: 1.5185495615005493\n",
            "Epoch: 23; Batch: 15; Loss: 1.5117334127426147\n",
            "Epoch: 23; Batch: 16; Loss: 1.5175620317459106\n",
            "Finished epoch 23.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = random.randint(0, len(test_ds))"
      ],
      "metadata": {
        "id": "kT2t4nqAKesS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input, label = test_ds[n]\n",
        "\n",
        "print(torch.argmax(model(torch.flatten(input, start_dim=-2))))\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mFGwJVNFLMA",
        "outputId": "cf694c99-3899-4ba7-be80-81bb9719c33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8)\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, label in test_loader:\n",
        "            inputs, label = inputs.to(device), label.to(device)\n",
        "            inputs = torch.flatten(inputs, start_dim=-3)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on 10000 test images: {100 * correct // total} %')\n",
        "    return 100 * correct // total"
      ],
      "metadata": {
        "id": "ViiyjQBBKvp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, test_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcI_PNhGKw50",
        "outputId": "694213aa-6404-4455-d793-8a7b8d4a99c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on 10000 test images: 94 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Layers\n",
        "\n",
        "We talked briefly about convolutional layers earlier, in Lecture 3.\n",
        "In particular, we talked about the Sobel operator and how that could help us detect edges.\n",
        "\n",
        "The convolutional layer as a neural network layer was first developed and used by Kunihiko Fukushima.\n",
        "He developed it for his handwriting recognition model called the **neocognitron**.\n",
        "The convolutional layer consists of several convolutional kernels that operate on the image.\n",
        "\n",
        "We'll run through an example by hand, then show how it looks on a few images.\n",
        "For a convolutional layer, you have three parameters:\n",
        "1.  number of kernels;\n",
        "2.  number of rows;\n",
        "3.  number of columns.\n",
        "\n",
        "Typically, we'll use a square matrix, which means that the number of rows and the number of columns are the same.\n",
        "We'll work with one $3 \\times 3$ kernel that operates on a $5 \\times 5$ matrix.\n",
        "\n",
        "Our $3 \\times 3$ kernel will be:\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}\n",
        "--1 & 0 & 1 \\\\\n",
        "-1 & 0 & 1 \\\\\n",
        "-1 & 0 & 1\n",
        "\\end{array}\n",
        "\\right)\n",
        "$$\n",
        "and our $5 \\times 5$ matrix will be:\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}\n",
        "44 & 4 & 2 & 4 & 4 \\\\\n",
        "5 & 3 & 4 & 0 & 3 \\\\\n",
        "1 & 0 & 2 & 3 & 0 \\\\\n",
        "5 & 4 & 2 & 4 & 5 \\\\\n",
        "0 & 0 & 4 & 3 & 2 \\\\\n",
        "\\end{array}\n",
        "\\right).\n",
        "$$\n",
        "\n",
        "The $3 \\times 3$ kernel is one of the Prewitt operators.\n",
        "Did you notice something about what happened to the size of the $5 \\times 5$ matrix after we did a convolution on it with the $3\\times 3$ kernel?\n",
        "The height and width both dropped by $2$.\n",
        "\n",
        "If we used\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}\n",
        "--1 & 0 & 1 \\\\\n",
        "-1 & 0 & 1 \\\\\n",
        "-1 & 0 & 1\n",
        "\\end{array}\n",
        "\\right)\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}\n",
        "--1 & -1 & -1 \\\\\n",
        "0 & 0 & 0 \\\\\n",
        "1 & 1 & 1\n",
        "\\end{array}\n",
        "\\right)\n",
        "$$\n",
        "as our convolutional kernels and we put in a $1 \\times 28 \\times 28$ MNIST image, we would get out a $2 \\times 26 \\times 26$ tensor.\n",
        "If we wanted to keep the same height and width, 28, then we can do something called padding.\n",
        "Padding is just adjoining different values to the edge of our input.\n",
        "\n",
        "Pytorch has a couple ways to do this padding operation, but the primary way we'll deal with this is as a parameter in our convolutional layer."
      ],
      "metadata": {
        "id": "nc952el70PvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_ds[5]\n",
        "\n",
        "# We choose 1. as the value below, because it's easier to see in the image that we padded it.\n",
        "padded_image = F.pad(image, pad=(1, 1, 1, 1), value=1.)\n",
        "padded_image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cryo5Z8LCC6",
        "outputId": "83dc38d4-0825-40fb-a3c0-09b0b5816edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 30, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(data, class_list=None):\n",
        "    image, label = data\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if class_list:\n",
        "        label = class_list[label]\n",
        "\n",
        "    plt.title(label)\n",
        "\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.imshow(image)"
      ],
      "metadata": {
        "id": "iPuiShzITptc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image((padded_image, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "8-DhoDLNTmEv",
        "outputId": "f4c9ca8e-c150-45a4-f67b-7462be08924c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_image((image, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "a8Sscap1TsK3",
        "outputId": "3777543c-6717-4eb1-92a0-5d9279c1d2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a closer look at the convolutional layer in PyTorch:\n",
        "\n",
        "`torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias)`\n",
        "\n",
        "Let's go through some of the parameters:\n",
        "\n",
        "*   `in_channels`: this is the number of channels in our input.\n",
        "So, if we input a $(3, 224, 224)$ tensor, we would set the value of this to 3.\n",
        "*   `out_channels`: this is the number of channels we want to ouptut.\n",
        "If we want to have 6 channels out, our output would be a $(6, x, y)$ tensor, then we set this to 6.\n",
        "*   `kernel_size`: this is the size of the kernel; above we had a $3 \\times 3$ kernel, so this would be set to 3.\n",
        "*   `stride`: this is how many elements to skip over; above we had a stride of 1.\n",
        "*   `padding`: we talked about this above.\n",
        "*   `bias`: this would be a tensor that has the same size as our out_channels; so, if we had `out_channels=6`, then this would be a $6$-tensor; this would be added to the output before its sent to the next layer.\n",
        "\n",
        "Let's look at an example:"
      ],
      "metadata": {
        "id": "cdS0-x1edcGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer = torch.nn.Conv2d(1, 6, 3, stride=1, padding=1, bias=True)"
      ],
      "metadata": {
        "id": "BlWX13jnUNac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer(image).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSCdEC2Nf5Rd",
        "outputId": "7f47a777-9de0-4114-cbda-90920b112a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaPw94ZOgnJX",
        "outputId": "0148f0bc-603b-4cd6-83f1-e6942bdeadc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-0.0673, -0.2265, -0.0031],\n",
              "          [-0.0041,  0.1653, -0.0317],\n",
              "          [ 0.0118,  0.0935,  0.2027]]],\n",
              "\n",
              "\n",
              "        [[[-0.0952,  0.0604, -0.0268],\n",
              "          [ 0.1381, -0.1519, -0.1449],\n",
              "          [-0.0346, -0.0587,  0.1010]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0330,  0.0489,  0.1120],\n",
              "          [-0.0314,  0.0447, -0.1251],\n",
              "          [-0.2172, -0.0571,  0.1003]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0071,  0.0843,  0.2853],\n",
              "          [-0.2690, -0.1519,  0.0256],\n",
              "          [-0.2291, -0.1051, -0.2674]]],\n",
              "\n",
              "\n",
              "        [[[-0.1899,  0.0865,  0.3299],\n",
              "          [-0.1899, -0.3073,  0.0016],\n",
              "          [ 0.2409, -0.1839, -0.3105]]],\n",
              "\n",
              "\n",
              "        [[[-0.2046, -0.2711, -0.1316],\n",
              "          [ 0.0644, -0.2850,  0.0753],\n",
              "          [-0.0336, -0.1795, -0.2756]]]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq9RehnFf6tx",
        "outputId": "f88c9047-0cb5-432f-cb8a-4e082cce491c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.0468, -0.1650,  0.0127, -0.2903,  0.2894, -0.0483],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have the first layer down.\n",
        "The next layer is what is called a pooling layer.\n",
        "There are typically two types of these layers: max pooling and average pooling.\n",
        "The purpose of these is to reduce the input size to the next layer in order to reduce the parameter size of the next layer.\n",
        "\n",
        "Let's talk about both max pooling and average pooling.\n",
        "For max pooling, our kernel just chooses the largest element in the kernel.\n",
        "With these operations, you should think of the kernel as a sliding window.\n",
        "Let's look at an example of this.\n",
        "We'll look at a $2 \\times 2$ and $3 \\times 3$ kernel on the matrix:\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}\n",
        "44 & 4 & 2 & 4 & 4 \\\\\n",
        "5 & 3 & 4 & 0 & 3 \\\\\n",
        "1 & 0 & 2 & 3 & 0 \\\\\n",
        "5 & 4 & 2 & 4 & 5 \\\\\n",
        "0 & 0 & 4 & 3 & 2 \\\\\n",
        "\\end{array}\n",
        "\\right).\n",
        "$$\n",
        "\n",
        "If we perform max pooling with a $2 \\times 2$ kernel we get:\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}\n",
        "55 & 4 & 4 & 4 \\\\\n",
        "5 & 4 & 4 & 3 \\\\\n",
        "5 & 4 & 4 & 5 \\\\\n",
        "5 & 4 & 4 & 5 \\\\\n",
        "\\end{array}\n",
        "\\right)\n",
        "$$\n",
        "and with a $3 \\times 3$ kernel, we get\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}\n",
        "55 & 4 & 4 \\\\\n",
        "5 & 4 & 4 \\\\\n",
        "5 & 4 & 5 \\\\\n",
        "\\end{array}\n",
        "\\right).\n",
        "$$\n",
        "\n",
        "This may seem tedious, but this is why we invented and use computers!\n"
      ],
      "metadata": {
        "id": "AcFmNfjcg1rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[\n",
        "    [4, 4, 2, 4, 4],\n",
        "    [5, 3, 4, 0, 3],\n",
        "    [1, 0, 2, 3, 0],\n",
        "    [5, 4, 2, 4, 5],\n",
        "    [0, 0, 4, 3, 2]\n",
        "]], dtype=torch.float)"
      ],
      "metadata": {
        "id": "3OXgQdbAf-jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.MaxPool2d(kernel_size=(2, 2), stride=1)(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSD6yMzklRnm",
        "outputId": "5140d671-9101-4699-fffe-e13a693f6093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[5., 4., 4., 4.],\n",
              "         [5., 4., 4., 3.],\n",
              "         [5., 4., 4., 5.],\n",
              "         [5., 4., 4., 5.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.MaxPool2d(kernel_size=(3, 3), stride=1)(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3SrXDcqlfq_",
        "outputId": "c84cfc49-7189-468a-a1a0-fa67f2f377bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[5., 4., 4.],\n",
              "         [5., 4., 5.],\n",
              "         [5., 4., 5.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at the average ppoling case on the same matrix.\n",
        "With a $2 \\times 2$ kernel we get:\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}\n",
        "44 & 3.25 & 2.5 & 2.75 \\\\\n",
        "2.25 & 2.75 & 2.25 & 1.5 \\\\\n",
        "2.5 & 2 & 2.75 & 3 \\\\\n",
        "2.5 & 2 & 2.75 & 3 \\\\\n",
        "\\end{array}\n",
        "\\right)\n",
        "$$\n",
        "with a $3 \\times 3$ kernel we get:\n",
        "$$\n",
        "\\left(\n",
        "\\begin{array}\n",
        "52.7778 & 2.4444 & 2.4444 \\\\\n",
        "2.8889 & 2.4444 & 2.5556 \\\\\n",
        "2.0000 & 2.4444 & 2.7778 \\\\\n",
        "\\end{array}\n",
        "\\right).\n",
        "$$"
      ],
      "metadata": {
        "id": "tzZiVWkelQUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.AvgPool2d(kernel_size=(3, 3,), stride=1)(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V_UGdYCibD7",
        "outputId": "2d6cd508-2efa-496b-ca99-a8aab7f8701b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[2.7778, 2.4444, 2.4444],\n",
              "         [2.8889, 2.4444, 2.5556],\n",
              "         [2.0000, 2.4444, 2.7778]]])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was much easier to calculate.\n",
        "\n",
        "Now, let's look at the PyTorch implementation of average pooling:\n",
        "`torch.nn.AvgPool2d(kernel_size, stride, padding)`\n",
        "*   `kernel_size`: this is just the sie of our sliding windows;\n",
        "*   `stride`: this is exactly the same as in the `torch.nn.Conv2d` case; it just tells us how many steps to take.\n",
        "*   `padding`: this is exactly the same as in the `torch.nn.Conv2d` case; again, it tells us how many zeros to pad our tensor with; this allows us to control the size of the output.\n",
        "\n",
        "`torch.nn.AvgPool2d` can be written as a regular convolutional layer."
      ],
      "metadata": {
        "id": "sSyjCyBKkiyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight = 1/9 * torch.ones((1, 1, 3, 3))"
      ],
      "metadata": {
        "id": "OvLGJdu1igXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.conv2d(x, weight=weight, stride=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLCQ9e93ln0d",
        "outputId": "4ad224d6-a7c3-4296-8b30-ba6263bac2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[2.7778, 2.4444, 2.4444],\n",
              "         [2.8889, 2.4444, 2.5556],\n",
              "         [2.0000, 2.4444, 2.7778]]])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we want a specific convolutional layer weight, we called `F.conv2d` as opposed to `torch.nn.Conv2d`.\n",
        "As you can see, we can do a convolution to get the same result as average pooling!\n",
        "\n",
        "We saw the `torch.flatten` layer above.\n",
        "Now, we're going to explain it.\n",
        "The function `torch.flatten` has three parameters:\n",
        "*   `input`: this is just the tensor that we want to flatten;\n",
        "*   `start_dim`: this is the start of the dimension we want to flatten;\n",
        "*   `end_dim`: this is the end of th edimensions we want to flatten.\n",
        "\n",
        "If we don't input anything for `start_dim` and `end_dim`, they default to flattening everything.\n",
        "\n",
        "Let's look at another example:"
      ],
      "metadata": {
        "id": "KL6gdtzMmaDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 4, 5, 6, 7, 8)"
      ],
      "metadata": {
        "id": "qdlzyxDKlpSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we call\n",
        "\n",
        "`torch.flatten(x, start_dim=1, end_dim=3)`\n",
        "\n",
        "then what will the shape of $x$ be?\n",
        "Let's take a minute to guess before we input it into the next cell."
      ],
      "metadata": {
        "id": "TMlMr0j9nIjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.flatten(x, start_dim=1, end_dim=3).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUWjxo3ZnGp3",
        "outputId": "12ffa9c2-c4a5-41f6-b5e4-a099a9adf94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 120, 7, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we talked about the stride of a tensor, so `torch.flatten` works by changing the stride of the tensor!\n",
        "\n",
        "So, the `x` is a $(3, 4, 5, 6, 7, 8)$ tensor, but the data is stored as a $20,160$ array, with stride $(6720, 1680, 336, 56, 8, 1)$."
      ],
      "metadata": {
        "id": "dCKKCt1SoFeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now we're going to make a 2-dimensional convolutional neural network and see how it does."
      ],
      "metadata": {
        "id": "oA_mSfjIpSoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvolutionalModel, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(1, 16, 5, padding=0)\n",
        "        self.conv2 = torch.nn.Conv2d(16, 16, 5, padding=0)\n",
        "        self.linear = torch.nn.Linear(16 * 20 * 20, 10)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.activation(x)\n",
        "        x = torch.flatten(x, start_dim=-3)\n",
        "        x = self.linear(x)\n",
        "        return self.softmax(x)"
      ],
      "metadata": {
        "id": "ICDvhRbRpd7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvolutionalModel()"
      ],
      "metadata": {
        "id": "3HnatjjapEDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)"
      ],
      "metadata": {
        "id": "WVVaOHuiplzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PhRLvEqqF88",
        "outputId": "9c99edbe-3f43-4ac6-f73e-8fc9c39ffaed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvolutionalModel(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (linear): Linear(in_features=6400, out_features=10, bias=True)\n",
              "  (activation): ReLU()\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    for batch, (inputs, label) in enumerate(train_dl):\n",
        "        inputs, label = inputs.to(device), label.to(device)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        out = model(inputs)\n",
        "        loss = criterion(out, label)\n",
        "        print(f\"Epoch: {epoch + 1}; Batch: {batch + 1}; Loss: {loss}\")\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    print(f\"Finished epoch {epoch + 1}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt2bW2_dreEe",
        "outputId": "88e4a1a8-1219-4e99-e416-d121e9057e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 1; Loss: 2.303826093673706\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 2; Loss: 2.298220157623291\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 3; Loss: 2.2871758937835693\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 4; Loss: 2.2620105743408203\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 5; Loss: 2.201171398162842\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 6; Loss: 2.060218334197998\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 7; Loss: 1.927082896232605\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 8; Loss: 2.077733278274536\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 9; Loss: 1.9426153898239136\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 10; Loss: 1.8411213159561157\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 11; Loss: 1.8501869440078735\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 12; Loss: 1.8627169132232666\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 13; Loss: 1.8347456455230713\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 14; Loss: 1.8432773351669312\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 15; Loss: 1.835569143295288\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 1; Batch: 16; Loss: 1.8212292194366455\n",
            "Finished epoch 1.\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 1; Loss: 1.8275190591812134\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 2; Loss: 1.8431144952774048\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 3; Loss: 1.8203579187393188\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 4; Loss: 1.8060330152511597\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 5; Loss: 1.8284364938735962\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 6; Loss: 1.8148481845855713\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 7; Loss: 1.81022047996521\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 8; Loss: 1.8110599517822266\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 9; Loss: 1.8193864822387695\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 10; Loss: 1.8350645303726196\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 11; Loss: 1.804317831993103\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 12; Loss: 1.8211512565612793\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 13; Loss: 1.8177872896194458\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 14; Loss: 1.814964771270752\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 15; Loss: 1.806233286857605\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 2; Batch: 16; Loss: 1.8186286687850952\n",
            "Finished epoch 2.\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 1; Loss: 1.809122920036316\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 2; Loss: 1.8018224239349365\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 3; Loss: 1.8130024671554565\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 4; Loss: 1.819931983947754\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 5; Loss: 1.7988617420196533\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 6; Loss: 1.8028796911239624\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 7; Loss: 1.8188719749450684\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 8; Loss: 1.8208321332931519\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 9; Loss: 1.8038731813430786\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 10; Loss: 1.7971493005752563\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 11; Loss: 1.8014500141143799\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 12; Loss: 1.798891305923462\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 13; Loss: 1.8025704622268677\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 14; Loss: 1.806699514389038\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 15; Loss: 1.8004746437072754\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 3; Batch: 16; Loss: 1.8107695579528809\n",
            "Finished epoch 3.\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 1; Loss: 1.8130371570587158\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 2; Loss: 1.791776418685913\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 3; Loss: 1.8140367269515991\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 4; Loss: 1.796569585800171\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 5; Loss: 1.8072577714920044\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 6; Loss: 1.7957844734191895\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 7; Loss: 1.8159056901931763\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 8; Loss: 1.798724889755249\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 9; Loss: 1.7938324213027954\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 10; Loss: 1.8033055067062378\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 11; Loss: 1.7980012893676758\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 12; Loss: 1.8021697998046875\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 13; Loss: 1.7974872589111328\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 14; Loss: 1.8065555095672607\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 15; Loss: 1.795805811882019\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 4; Batch: 16; Loss: 1.8006430864334106\n",
            "Finished epoch 4.\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 1; Loss: 1.7922344207763672\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 2; Loss: 1.7949341535568237\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 3; Loss: 1.7957960367202759\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 4; Loss: 1.8037824630737305\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 5; Loss: 1.8076326847076416\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 6; Loss: 1.793506383895874\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 7; Loss: 1.8013641834259033\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 8; Loss: 1.800696611404419\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 9; Loss: 1.7900007963180542\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 10; Loss: 1.7957074642181396\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 11; Loss: 1.7908923625946045\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 12; Loss: 1.8009812831878662\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 13; Loss: 1.8101283311843872\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 14; Loss: 1.7908620834350586\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 15; Loss: 1.781046748161316\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 5; Batch: 16; Loss: 1.7961679697036743\n",
            "Finished epoch 5.\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 1; Loss: 1.8048750162124634\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 2; Loss: 1.7919728755950928\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 3; Loss: 1.7929519414901733\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 4; Loss: 1.8093442916870117\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 5; Loss: 1.7941893339157104\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 6; Loss: 1.7828117609024048\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 7; Loss: 1.7846014499664307\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 8; Loss: 1.784192681312561\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 9; Loss: 1.8117752075195312\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 10; Loss: 1.8031010627746582\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 11; Loss: 1.7884626388549805\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 12; Loss: 1.7996320724487305\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 13; Loss: 1.7786579132080078\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 14; Loss: 1.806850552558899\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 15; Loss: 1.7911031246185303\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 6; Batch: 16; Loss: 1.7990916967391968\n",
            "Finished epoch 6.\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 1; Loss: 1.7916334867477417\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 2; Loss: 1.8050682544708252\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 3; Loss: 1.7864612340927124\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 4; Loss: 1.7874475717544556\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 5; Loss: 1.7889800071716309\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 6; Loss: 1.7954202890396118\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 7; Loss: 1.8023805618286133\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 8; Loss: 1.8023350238800049\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 9; Loss: 1.7938969135284424\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 10; Loss: 1.7867114543914795\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 11; Loss: 1.795467495918274\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 12; Loss: 1.8024908304214478\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 13; Loss: 1.7830346822738647\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 14; Loss: 1.8014817237854004\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 15; Loss: 1.7999681234359741\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 7; Batch: 16; Loss: 1.7807862758636475\n",
            "Finished epoch 7.\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 1; Loss: 1.7878296375274658\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 2; Loss: 1.7868098020553589\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 3; Loss: 1.78325355052948\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 4; Loss: 1.7911752462387085\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 5; Loss: 1.7941207885742188\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 6; Loss: 1.802140712738037\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 7; Loss: 1.7909420728683472\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 8; Loss: 1.7937302589416504\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 9; Loss: 1.7972795963287354\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 10; Loss: 1.7914727926254272\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 11; Loss: 1.7879235744476318\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 12; Loss: 1.7965775728225708\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 13; Loss: 1.7837644815444946\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 14; Loss: 1.78599214553833\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 15; Loss: 1.7842615842819214\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 8; Batch: 16; Loss: 1.7901297807693481\n",
            "Finished epoch 8.\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 1; Loss: 1.7771183252334595\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 2; Loss: 1.794005036354065\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 3; Loss: 1.7925719022750854\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 4; Loss: 1.7979843616485596\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 5; Loss: 1.7981902360916138\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 6; Loss: 1.7824472188949585\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 7; Loss: 1.7905137538909912\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 8; Loss: 1.7783458232879639\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 9; Loss: 1.8040261268615723\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 10; Loss: 1.7759814262390137\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 11; Loss: 1.7765172719955444\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 12; Loss: 1.785178780555725\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 13; Loss: 1.7746102809906006\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 14; Loss: 1.7966275215148926\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 15; Loss: 1.7968698740005493\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 9; Batch: 16; Loss: 1.778682827949524\n",
            "Finished epoch 9.\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 1; Loss: 1.7899446487426758\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 2; Loss: 1.7966911792755127\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 3; Loss: 1.7690613269805908\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 4; Loss: 1.7970572710037231\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 5; Loss: 1.790095329284668\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 6; Loss: 1.7876988649368286\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 7; Loss: 1.7868579626083374\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 8; Loss: 1.774240493774414\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 9; Loss: 1.7699583768844604\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 10; Loss: 1.777076244354248\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 11; Loss: 1.7875428199768066\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 12; Loss: 1.7922136783599854\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 13; Loss: 1.7871720790863037\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 14; Loss: 1.787449598312378\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 15; Loss: 1.7826530933380127\n",
            "torch.Size([3750, 1, 28, 28])\n",
            "Epoch: 10; Batch: 16; Loss: 1.7904754877090454\n",
            "Finished epoch 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = random.randint(0, len(test_ds))\n",
        "\n",
        "input, label = test_ds[n]\n",
        "\n",
        "print(torch.argmax(model(input)))\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgKz1NxJsM_J",
        "outputId": "162743ef-9410-4d8f-fb56-c619a6ae662f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4)\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, label in test_loader:\n",
        "            inputs, label = inputs.to(device), label.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on 10000 test images: {100 * correct / total:.3f} %')\n",
        "    return 100 * correct // total"
      ],
      "metadata": {
        "id": "KdidiYMftqMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_evaluate(model, test_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzpmAInIrkdG",
        "outputId": "4c8203fb-5f08-439f-a22b-7bd7def22dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on 10000 test images: 68.070 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    }
  ]
}