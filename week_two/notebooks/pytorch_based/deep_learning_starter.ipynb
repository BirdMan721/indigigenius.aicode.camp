{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df77e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20b68f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import shutil\n",
    "import tarfile\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea0a95cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(data, class_list=None):\n",
    "    image, label = data\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "        \n",
    "    if class_list:\n",
    "        label = class_list[label]\n",
    "        \n",
    "    plt.title(label)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb601fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_history(history):\n",
    "    training_steps = np.array([step for step, _ in history])\n",
    "    loss = np.array([loss for _, loss in history])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    ax.plot(training_steps, loss)\n",
    "\n",
    "    ax.set(xlabel='training_steps', ylabel='loss')\n",
    "    ax.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7be0be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, test_loader):\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        t = time.time()\n",
    "        for i, (inputs, label) in enumerate(test_loader):\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            print(label.size(0))\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            \n",
    "            t = time.time() - t\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(\"Batch     | Time(s)\")\n",
    "            print(\"-------------------\")\n",
    "            print(f\"{i + 1:5d} / {len(test_loader):5d} | {int(t):7d}\")\n",
    "\n",
    "            \n",
    "    accuracy = 100 * correct / total\n",
    "            \n",
    "    print(f'Accuracy of the network on {len(test_loader.dataset)} test images: {accuracy:.1f} %')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a25aef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.1, use_cuda=True):\n",
    "    # This is code from the authors of the paper\n",
    "    # https://arxiv.org/abs/1710.09412\n",
    "    \n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        # Note, this means that MixUp does nothing.\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size()[0]\n",
    "    \n",
    "    if use_cuda:\n",
    "        # This shuffles our indices and sends these indices to the GPU\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        # This shuffles our indices without sending them to the GPU\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62225ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, dataset, epoch, opt, scheduler, loss_history=None):\n",
    "    \n",
    "    model_name = model.__class__.__name__\n",
    "    dataset_name = dataset.__class__.__name__\n",
    "    t = int(time.time())\n",
    "    \n",
    "    if dataset_name:\n",
    "        ckpt_path = '/workspace/models/' + model_name + '_' + dataset_name + f\"_{t}.ckpt\"\n",
    "    else:\n",
    "        ckpt_path = '/workspace/models/' + model_name + f\"_{t}.ckpt\"\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'loss_history': loss_history\n",
    "            }, ckpt_path)\n",
    "\n",
    "    return ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4af997a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path):\n",
    "    \n",
    "    ckpt_dict = torch.load(path)\n",
    "    \n",
    "    model.load_state_dict(ckpt_dict['model_state_dict'])\n",
    "    \n",
    "    return ckpt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd5b1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, image, transforms, device=None, dataset=None):\n",
    "    # Image input is assumed to be an image of height, width, channel.\n",
    "    # Class list has a list of the objects classification.\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    image = transforms(image)\n",
    "    image.to(device)\n",
    "    if len(image.shape) < 4:\n",
    "        image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    pred = model(image)\n",
    "    pred = torch.nn.Softmax(dim=1)(pred)\n",
    "    pred = pred.squeeze(0)\n",
    "    confidence = pred\n",
    "    pred = torch.argmax(pred).item()\n",
    "    \n",
    "    final_pred = (dataset.classes[pred], confidence[pred].item()) if dataset else (pred, confidence[pred].item())\n",
    "    \n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ccacb768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_batch_train_mixup(model, images, label, use_cuda, alpha=0.1):\n",
    "    mixed_x, y_a, y_b, lam = mixup_data(images, label, alpha=alpha, use_cuda=use_cuda)\n",
    "    pred = model(mixed_x)\n",
    "\n",
    "    loss = mixup_criterion(criterion, pred, y_a, y_b, lam)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "daa744e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, opt, train_dl, test_dl, start_epoch=0, scheduler=None,\n",
    "          mixup=False, mixup_pct=0.0, use_amp=False, checkpoint=0, loss_history=None,\n",
    "          use_swa=False, swa_start=0, swa_lr=3e-4, epochs=23, evaluate=True):\n",
    "    \n",
    "    # The history of our training, inputs step, loss pair.\n",
    "    \n",
    "    history = loss_history if loss_history else []\n",
    "    \n",
    "    # Initialize step\n",
    "    step = 1\n",
    "    \n",
    "    # This is the code that allows us to use our GPU to train.\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    if use_swa:\n",
    "        swa_scheduler = torch.optim.swa_utils.SWALR(opt, anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=swa_lr)\n",
    "        swa_model = torch.optim.swa_utils.AveragedModel(model)\n",
    "    \n",
    "    # This controls how many iterations we want to make, which helps control how well our model\n",
    "    # generalizes.\n",
    "    total_time = time.time()\n",
    "    \n",
    "    if use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # Initializing the time for informational purposes.\n",
    "        t = time.time()\n",
    "        batch_start = t\n",
    "\n",
    "        #This is how we get our inputs and labels from our dataloader.\n",
    "        for i, (images, label) in enumerate(train_dl):\n",
    "            # We have to zero out our gradients, because they can get too large.\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            # This sends the inputs and the labels to our device.\n",
    "            images, label = images.to(device), label.to(device)\n",
    "            \n",
    "            # This is the prediction of our model\n",
    "            if mixup and random.random() < mixup_pct:\n",
    "                loss = one_batch_train_mixup(model, images, label, use_cuda=use_cuda, alpha=0.1)\n",
    "            else:\n",
    "                pred = model(images)\n",
    "            \n",
    "                # Remember loss functions take two inputs, the prediction and the label.\n",
    "                loss = criterion(pred, label)\n",
    "            \n",
    "            # This step gets our gradients.\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                \n",
    "            if use_swa and epoch > swa_start:\n",
    "                swa_model.update_parameters(model)\n",
    "                swa_scheduler.step()\n",
    "            else:\n",
    "                # This is how we update our neural network weights. \n",
    "                opt.step()\n",
    "            \n",
    "            # Append current step and loss.\n",
    "            history.append((step, loss.item()))\n",
    "            step += 1\n",
    "\n",
    "            # Get time Batch training is taking.\n",
    "            t = time.time() - batch_start\n",
    "\n",
    "            # Gives a nice representation of how our training is progressing.\n",
    "            clear_output(wait=True)\n",
    "            print(\"Epoch | Batch | Time(s) | Loss\")\n",
    "            print(\"------------------------------\")\n",
    "            print(f\"{epoch + 1:5d} | {i + 1:5d} | {int(t):7d} | {loss:.5f}\")\n",
    "        \n",
    "        print(f\"Finished epoch {epoch + 1}.\")\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if checkpoint and ((epoch + 1) % checkpoint == 0):\n",
    "            save_model(model, train_dl.dataset, epoch, opt, scheduler, loss_history=history)\n",
    "        \n",
    "    if use_swa and epochs > swa_start:\n",
    "        torch.optim.swa_utils.update_bn(train_dl, model, device=device)\n",
    "    \n",
    "    total_time = time.time() - total_time\n",
    "    print(f\"Finished Training in {int(total_time)} seconds!\")\n",
    "    if evaluate:\n",
    "        print(\"Evaluating\")\n",
    "        model_evaluate(model, test_dl)\n",
    "        \n",
    "    print(f\"Saving model!\")\n",
    "    save_model(model, train_dl.dataset, epoch, opt, scheduler, loss_history=history)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "29fad04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_onnx(model, input_shape, path=None):\n",
    "    bs, c, h, w = input_shape\n",
    "    \n",
    "    dummy_input = torch.randn(bs, c, h, w, device='cuda')\n",
    "    \n",
    "    model.to('cuda')\n",
    "    \n",
    "    input_names = [ \"input\" ]\n",
    "    output_names = [ \"output\" ]\n",
    "    \n",
    "    if not path:\n",
    "        path = model.__class__.__name__ + '.onnx'\n",
    "    \n",
    "    torch.onnx.export(model, dummy_input, path, verbose=True, input_names=input_names, output_names=output_names)\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9127a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset, transform, target_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform=target_transform\n",
    "        self.name = 'Lakota_Plants'\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        label = self.dataset[idx][1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801aa846",
   "metadata": {},
   "source": [
    "# Extract the Training and Test Data\n",
    "\n",
    "Currently we only have training data, but this will be extracted in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if plant_data is made.\n",
    "if not os.path.isdir('/workspace/data/plant_data/'):\n",
    "    os.mkdir('/workspace/data/plant_data')\n",
    "\n",
    "# Extract data.\n",
    "with tarfile.open(\"/workspace/data/2022_laicc_plant_data.tar.gz\", 'r:gz') as tar:\n",
    "    tar.extractall('/workspace/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e5ad9",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "These are the variables that you will change to make your model run better or faster.\n",
    "All of your hyperparameters are below and you will make changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "de41047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'lr': 3e-3,\n",
    "    'num_classes': 6, # Don't change\n",
    "    # Decide whether you want to add mixup training and how often it is run..\n",
    "    'mixup': True,\n",
    "    'mixup_pct': 0.90,\n",
    "    # Automatic Mixed Precision, should speed up training.\n",
    "    'use_amp': True,\n",
    "    # Epochs - how long you want to train\n",
    "    'epochs': 15,\n",
    "    'start_epoch': 0,\n",
    "    # Pin the memory, this should speed up training, but could make the kernel more stable\n",
    "    'pin_memory': True, # Don't change\n",
    "    # This is your batch size.\n",
    "    'bs': 16,\n",
    "    # Whether or not to use stochastic weight averaging. Setting to true should increase test accuracy.\n",
    "    # If use_swa is set to False, then swa_start and swa_lar is not used.\n",
    "    'use_swa': False,\n",
    "    'swa_start': 12,\n",
    "    'swa_lr': 5e-3,\n",
    "    # How many epochs to train before saving model. If set to 0, checkpointing will not be performed.\n",
    "    'checkpoint': 5,\n",
    "    # Where your training data is stored.\n",
    "    'train_root': '/workspace/data/plant_data/', # Do not change\n",
    "    # Where your test data is stored.\n",
    "    'test_root': '/workspace/data/plant_data/', # Do not change\n",
    "    # This should be a number and how often you should save your data.\n",
    "    'num_workers': 4,\n",
    "    # If you want to restart training, change this to the path of the checkpoint you wish to start at.\n",
    "    'ckpt_path': None, #'/workspace/models/ResNet_Lakota_Plants_1658958130.ckpt',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7679cf6",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "\n",
    "Define your transforms below. The minimal transform input is included below, but you can and should add more.\n",
    "Select new transforms from [Pytorch Transforms](https://pytorch.org/vision/main/transforms.html#transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fedaedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose(\n",
    "    [T.Resize(size=(246, 246)),\n",
    "     T.CenterCrop([224]),\n",
    "     T.AugMix(),\n",
    "     #T.RandomPerspective(),\n",
    "     #T.RandomHorizontalFlip(p=0.5),\n",
    "     #T.RandomVerticalFlip(p=0.5),\n",
    "     #T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1)),\n",
    "     #T.RandomRotation(degrees=(0, 10)),\n",
    "     T.Grayscale(num_output_channels=3),\n",
    "     T.ToTensor(),\n",
    "     T.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]), std=torch.tensor([0.229, 0.224, 0.225])),\n",
    "    ])\n",
    "\n",
    "test_transforms = T.Compose(\n",
    "    [T.Resize(size=(246, 246)),\n",
    "     T.CenterCrop([224]),\n",
    "     T.Grayscale(num_output_channels=3),\n",
    "     T.ToTensor(),\n",
    "     T.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]), std=torch.tensor([0.229, 0.224, 0.225])),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33800b8b",
   "metadata": {},
   "source": [
    "# Initialize the Dataset\n",
    "\n",
    "We will initialize our dataset using the transforms above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "71fa3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your training dataset by setting train=True\n",
    "base_ds = torchvision.datasets.ImageFolder(root=hparams['train_root'], transform=None,\n",
    "                                            target_transform=None, loader=Image.open,\n",
    "                                            is_valid_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b2b88008",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split = torch.utils.data.random_split(base_ds, [len(base_ds) - 50, 50],\n",
    "                                                  generator=torch.Generator().manual_seed(38))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f9d39e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TransformDataset(train_split, transform=transforms)\n",
    "\n",
    "test_ds = TransformDataset(test_split, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6eadf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.__class__.__name__ = 'Lakota_Plants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "43b27661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [0 for _ in range(len(base_ds.classes))]\n",
    "\n",
    "for _, label in train_split:\n",
    "    class_weights[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ba62bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These class weights are based on torch.Generator().manual_seed(38)\n",
    "\n",
    "class_weights = 1 / torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6e72bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampler = torch.utils.data.WeightedRandomSampler(class_weights, 5000, replacement=True, generator=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a409dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your dataloader\n",
    "train_dl = torch.utils.data.DataLoader(train_ds,\n",
    "                                       batch_size=hparams['bs'],\n",
    "                                       shuffle=False,\n",
    "                                       sampler=weighted_sampler,\n",
    "                                       num_workers=hparams['num_workers'],\n",
    "                                       persistent_workers=True,\n",
    "                                       pin_memory=hparams['pin_memory'])\n",
    "test_dl = torch.utils.data.DataLoader(test_ds,\n",
    "                                      batch_size=hparams['bs'],\n",
    "                                      shuffle=False,\n",
    "                                      sampler=None,\n",
    "                                      num_workers=hparams['num_workers'],\n",
    "                                      persistent_workers=False,\n",
    "                                      pin_memory=hparams['pin_memory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "96e69aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your model from the following link:\n",
    "# https://pytorch.org/vision/stable/models.html#classification\n",
    "\n",
    "model = torchvision.models.vit_b_16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ca3e8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.heads.head = torch.nn.Linear(model.heads.head.in_features, hparams['num_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a684ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if not 'heads' in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b04db266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are doing classification, we will always use this loss.\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "30249e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=hparams['lr'], momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(opt, base_lr=hparams['lr'], max_lr=10*hparams['lr'],\n",
    "                                          step_size_up=1000, mode='triangular',\n",
    "                                          cycle_momentum=False)\n",
    "#scheduler2 = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.95)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.ChainedScheduler([scheduler1, scheduler2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c3636",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "You can select one of the following optimizers.\n",
    "For all of these, `params=model.parameters()`\n",
    "\n",
    "- `torch.optim.Adadelta(params, lr=1, rho=0.9, eps=1e-06, weight_decay=0)`\n",
    "- `torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, eps=1e-10)`\n",
    "- `torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)` use lr=3e-4 or close to it for Adam.\n",
    "- `torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)`\n",
    "- `torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)`\n",
    "- `torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0)`\n",
    "- `torch.optim.SGD(params, lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a7d04b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if hparams['ckpt_path']:\n",
    "    ckpt_dict = torch.load(hparams['ckpt_path'])\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.load_state_dict(ckpt_dict['model_state_dict'], strict=False)\n",
    "    opt.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(ckpt_dict['scheduler'])\n",
    "    hparams['start_epoch'] = ckpt_dict['epoch']\n",
    "    for state in opt.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "    \n",
    "else:\n",
    "    ckpt_dict = {\n",
    "            'epoch': 0,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'loss_history': None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ce95b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Batch | Time(s) | Loss\n",
      "------------------------------\n",
      "   15 |    12 |     111 | 0.00750\n"
     ]
    }
   ],
   "source": [
    "# Call to begin training\n",
    "# Change the value of epochs\n",
    "history = train(model, criterion, opt, train_dl, test_dl, start_epoch=hparams['start_epoch'], \n",
    "                scheduler=scheduler, mixup=hparams['mixup'], mixup_pct=hparams['mixup_pct'],\n",
    "                use_amp=hparams['use_amp'], checkpoint=hparams['checkpoint'],\n",
    "                loss_history=ckpt_dict['loss_history'], use_swa=hparams['use_swa'],\n",
    "                swa_start=hparams['swa_start'], swa_lr=hparams['swa_lr'],\n",
    "                epochs=hparams['epochs'], evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6242ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "\n",
    "for _, label in test_split:\n",
    "    y_true.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "143fd362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4a6fabd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "model.to('cuda')\n",
    "\n",
    "for image, _ in test_dl:\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    image = image.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = torch.nn.Softmax(dim=1)(model(image))\n",
    "        labels = torch.argmax(preds, dim=1)\n",
    "        labels = torch.flatten(labels)\n",
    "        labels = [l.item() for l in labels]\n",
    "        y_pred.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7e5d3f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [item for sublist in y_pred for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7f620a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4e7b73df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e4c0656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ceeb6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7df6e25c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x936 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n",
    "                              display_labels=base_ds.classes)\n",
    "                              #display_labels=[i for i in range(len(base_ds.classes))])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,13))\n",
    "disp.plot(ax=ax)\n",
    "plt.savefig('cfm_resnext_lakota_plant_data.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5cbbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make a prediction, paste the path to the image below.\n",
    "# # img_path = '/workspace/data/plant_data/Purple_Coneflower/20220716_230129.jpg'\n",
    "\n",
    "# img = Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2fc373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_prediction(model.to('cuda'), img, test_transforms, device='cuda', dataset=base_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be554fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    model,\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF you are satisfied with your models performance, save to ONNX\n",
    "# You need to change your input shape. It should be of the form\n",
    "# (batch_size, num_channels, height, width)\n",
    "# You will get height and width from the transform\n",
    "# T.Resize(size=(224, 224)), which would mean we would have\n",
    "input_shape = (10, 3, 224, 224)\n",
    "\n",
    "\n",
    "path = f'/workspace/models/{model.__class__.__name__}_{int(time.time())}.onnx'\n",
    "\n",
    "save_to_onnx(model, input_shape, path=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
