{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e5b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85089653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202247ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(data, class_list=None):\n",
    "    image, label = data\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "        \n",
    "    if class_list:\n",
    "        label = class_list[label]\n",
    "        \n",
    "    plt.title(label)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9291e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_history(history):\n",
    "    training_steps = np.array([step for step, _ in history])\n",
    "    loss = np.array([loss for _, loss in history])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(training_steps, loss)\n",
    "\n",
    "    ax.set(xlabel='training_steps', ylabel='loss')\n",
    "    ax.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea25178",
   "metadata": {},
   "source": [
    "Our breakdown of machine learning was:\n",
    "- data\n",
    "- model\n",
    "- training.\n",
    "\n",
    "We're going to go into more detail on these:\n",
    "- data\n",
    "    - dataset\n",
    "    - dataloader\n",
    "    - data augmentation\n",
    "- model\n",
    "    - neural network layer\n",
    "    - activation function\n",
    "- training\n",
    "    - loss function\n",
    "    - optimizer\n",
    "\n",
    "We covered all of data.\n",
    "We're going to go into the training function from earlier!\n",
    "Afterwards, we'll talk more about activation functions, loss functions, and optimizers.\n",
    "\n",
    "Remember our training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e910f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, opt, train_dl, test_dl, epochs=23, evaluate=True):\n",
    "    \n",
    "    # The history of our training, inputs step, loss pair.\n",
    "    history = []\n",
    "    \n",
    "    # Initialize step\n",
    "    step = 1\n",
    "\n",
    "    # This is our loss function\n",
    "    # criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    # This is our optimizer\n",
    "    # opt = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "    \n",
    "    # This is the code that allows us to use our GPU to train.\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # This controls how many iterations we want to make, which helps control how well our model\n",
    "    # generalizes.\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        t = time.time()\n",
    "        batch_start = t\n",
    "\n",
    "        #This is how we get our inputs and labels from our dataloader.\n",
    "        for i, (inputs, label) in enumerate(train_dl):\n",
    "            \n",
    "            # This sends the inputs and the labels to our device.\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "            # We have to zero out our gradients, because they can get too large.\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # This is the prediction of our model\n",
    "            #out = torch.nn.LogSoftmax(dim=1)(model(inputs))\n",
    "            out = model(inputs)\n",
    "            \n",
    "            # Remember loss functions take two inputs, the prediction and the label.\n",
    "            loss = criterion(out, label)\n",
    "            \n",
    "            # This step gets our gradients.\n",
    "            loss.backward()\n",
    "            \n",
    "            # This is how we update our neural network weights. \n",
    "            opt.step()\n",
    "            \n",
    "            # Append current step and loss.\n",
    "            history.append((step, loss.item()))\n",
    "            step += 1\n",
    "\n",
    "            # Get time Batch training is taking.\n",
    "            t = time.time() - batch_start\n",
    "\n",
    "            # Gives a nice representation of how our training is progressing.\n",
    "            clear_output(wait=True)\n",
    "            print(\"Epoch | Batch | Time(s) | Loss\")\n",
    "            print(\"------------------------------\")\n",
    "            print(f\"{epoch + 1:5d} | {i:5d} | {int(t):7d} | {loss:.5f}\")\n",
    "        \n",
    "        print(f\"Finished epoch {epoch + 1}.\")\n",
    "    print(f\"Finished Training!\")\n",
    "    if evaluate:\n",
    "        print(\"Evaluating\")\n",
    "        model_evaluate(model, test_dl)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5550d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, test_loader):\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, label in test_loader:\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            \n",
    "    print(f'Accuracy of the network on 10000 test images: {100 * correct // total} %')\n",
    "    return 100 * correct // total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436a9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize(mean=torch.tensor([0.1307]), std=torch.tensor([0.3081]))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08468e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_ds = torchvision.datasets.CIFAR10(root='../data',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transforms)\n",
    "\n",
    "test_ds = torchvision.datasets.CIFAR10(root='../data',\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0251b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the batch size until your GPU near's capacity.\n",
    "# We can check this by running `nvidia-smi` in Windows PowerShell\n",
    "bs = 2048\n",
    "\n",
    "# Think about how many training steps you want your model to take.\n",
    "# Typically this is a function of dataset size, batch size, and epochs.\n",
    "# The number of steps is: ((len(dataset) // batch_size) + 1) * epochs\n",
    "# So, if we had 50,000 elements in our dataset, a batch size of 1024, and 23 epochs, then \n",
    "# we would take 1,127 training steps.\n",
    "# In industry, the models can train for hundreds of thousands and even millions of steps.\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c6f20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds,\n",
    "                                       batch_size=bs,\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=4)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds,\n",
    "                                      batch_size=bs,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "af38a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your model here\n",
    "# model = LeNet()\n",
    "model = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "996bf49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(model.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "39888b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "19978ee1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Batch | Time(s) | Loss\n",
      "------------------------------\n",
      "  500 |    24 |      11 | 0.00000\n",
      "Finished epoch 500.\n",
      "Finished Training!\n",
      "Evaluating\n",
      "Accuracy of the network on 10000 test images: 79 %\n"
     ]
    }
   ],
   "source": [
    "history = train(model, criterion, opt, train_dl, test_dl, epochs=500, evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a00415af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ca34560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on 10000 test images: 79 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluate(model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa66a6",
   "metadata": {},
   "source": [
    "Now we save our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c182e31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Naming convention\n",
    "# ../models/model_name_TIME_dataset.pt\n",
    "\n",
    "PATH = '../models/resnet50_20220720_CIFAR10.pt'\n",
    "\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cad606fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../models/resnet50_20220720_CIFAR10.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69e1d0",
   "metadata": {},
   "source": [
    "We load our model in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b8d6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insantiate the correct model class\n",
    "model = torchvision.models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97bb507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(model.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc6991e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4508e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on 10000 test images: 9 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluate(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2208aefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2e88bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on 10000 test images: 79 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluate(model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae21d3c",
   "metadata": {},
   "source": [
    "Ok, I'm going to give you a list of loss functions and a list of optimizers.\n",
    "You are then going to fiddle around with the hyperparameters and see if you can get your model to train more quickly.\n",
    "\n",
    "**Criterion**\n",
    "- `torch.nn.MSELoss()`\n",
    "- `torch.nn.NLLLoss()`\n",
    "- `torch.nn.L1Loss()`\n",
    "\n",
    "**Optimizers**\n",
    "- `torch.optim.Adadelta(params, lr=1, rho=0.9, eps=1e-06, weight_decay=0)`\n",
    "- `torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, eps=1e-10)`\n",
    "- `torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)` **NOTE**: I will tell you the most commonly used value for `lr`.\n",
    "- `torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)`\n",
    "- `torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)`\n",
    "- `torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0)`\n",
    "- `torch.optim.SGD(params, lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False)`\n",
    "\n",
    "We'll go through these on the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some of the transforms, that we talked about in class number 5, to the list.\n",
    "# T.ToTensor and T.Normalize must remain.\n",
    "\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [T.ToTensor(),\n",
    "     T.Normalize(mean=torch.tensor([0.1307]), std=torch.tensor([0.3081])),\n",
    "     # Add the new transforms here.\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torchvision.datasets.MNIST(root='./data',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transforms)\n",
    "\n",
    "test_ds = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your batch size\n",
    "# bs = ..\n",
    "\n",
    "# Call your data.\n",
    "train_dl = torch.utils.data.DataLoader(train_ds,\n",
    "                                       batch_size=bs,\n",
    "                                       shuffle=True,\n",
    "                                       num_workers=4)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds,\n",
    "                                      batch_size=bs,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d124d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your model\n",
    "# model = ...\n",
    "\n",
    "# Select your loss function\n",
    "# criterion = ...\n",
    "\n",
    "# Input the hyperparameters for your optimizer.\n",
    "# Note, if you're note sure what hyperparameters to use for your optimizer, choose ones\n",
    "# near the default values.\n",
    "# Be sure to insert model.parameters() in the first slot!\n",
    "# opt = torch.optim....\n",
    "\n",
    "# choose your epochs and train your model!\n",
    "# train(model, criterion, opt, train_dl, test_dl, epochs, evaluate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353cf461",
   "metadata": {},
   "source": [
    "We're not going to upload an image to our images folder, then open it to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34954e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('./images/your_pic.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c366743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image, class_list=None):\n",
    "    \n",
    "    image = T.Resize((28, 828))(image)\n",
    "    image = T.ToTensor()(image)\n",
    "    \n",
    "    pred = model(image)\n",
    "    pred = torch.argmax(pred).item()\n",
    "    \n",
    "    if class_list:\n",
    "        pred = class_list[pred]\n",
    "    \n",
    "    return pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
